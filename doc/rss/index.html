<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Re Alvarez Parmar's Blog]]></title><description><![CDATA[I am writer, coder, reader, and father of three kids. I write about Cloud, Kubernetes, Containers, Blockchain, web3, and psychology.]]></description><link>https://realvz.github.io/blog/</link><image><url>https://realvz.github.io/blog/favicon.png</url><title>Re Alvarez Parmar&apos;s Blog</title><link>https://realvz.github.io/blog/</link></image><generator>Ghost 5.79</generator><lastBuildDate>Sat, 16 Mar 2024 19:37:59 GMT</lastBuildDate><atom:link href="https://realvz.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Cert-Manager: Encrypt Kubernetes service to service communication with self-signed certificates]]></title><description><![CDATA[<p>Cert-manager is an open-source certificate management solution for Kubernetes clusters. It automates the issuance, renewal, and management of SSL/TLS certificates within a Kubernetes environment. Cert-manager is crucial in securing communication between services, encrypting HTTP traffic, and establishing secure connections to external systems.</p><p>Cert-manager integrates with popular certificate authorities (CAs)</p>]]></description><link>https://realvz.github.io/blog/cert-manager-encrypt-kubernetes-service-to-service-communication-with-self-signed-certificates/</link><guid isPermaLink="false">64962a7c70f27a0001e12902</guid><category><![CDATA[Kubernetes]]></category><category><![CDATA[Security]]></category><category><![CDATA[cert-manager]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Fri, 23 Jun 2023 23:41:23 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1533922829859-83c5b158a336?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fGVuY3J5cHRpb24lMjBpbiUyMG1vdGlvbnxlbnwwfHx8fDE2ODc1NjM2MTJ8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1533922829859-83c5b158a336?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fGVuY3J5cHRpb24lMjBpbiUyMG1vdGlvbnxlbnwwfHx8fDE2ODc1NjM2MTJ8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Cert-Manager: Encrypt Kubernetes service to service communication with self-signed certificates"><p>Cert-manager is an open-source certificate management solution for Kubernetes clusters. It automates the issuance, renewal, and management of SSL/TLS certificates within a Kubernetes environment. Cert-manager is crucial in securing communication between services, encrypting HTTP traffic, and establishing secure connections to external systems.</p><p>Cert-manager integrates with popular certificate authorities (CAs) such as <a href="https://letsencrypt.org/?ref=blog.realvarez.com">Let&apos;s Encrypt</a>, <a href="https://venafi.com/?utm_adgroup=131519153019&amp;gclid=CjwKCAjwhdWkBhBZEiwA1ibLmO96fec69F8BPVPLODTo-D68AsjnSZfHFE_HRF56FDxXzcxtFUM3WxoCMvQQAvD_BwE&amp;utm_medium=cpc&amp;utm_term=venafi&amp;utm_content=hp&amp;utm_source=gog&amp;utm_campaign=15633869885">Venafi</a>, and <a href="https://www.vaultproject.io/?ref=blog.realvarez.com">HashiCorp Vault</a> to get certificates. It acts as a bridge between Kubernetes and these CAs, handling the certificate lifecycle management process.</p><p>I recently helped a customer create a self-signed CA. While researching, I found that there were no examples showing how to use self-signed private CA with cert-manager. There are plenty of examples about Let&apos;s Encrypt, Venafi, and <a href="https://aws.amazon.com/private-ca/?ref=blog.realvarez.com">AWS ACM Private CA</a>. But none when you want to issue client certificates using a self-signed CA. This post shows how to create a private CA in cert-manager and generate client certificates using <a href="https://cert-manager.io/docs/usage/csi/?ref=blog.realvarez.com">cert-manager CSI driver</a>.</p><h2 id="setup-cert-manager">Setup cert-manager</h2><p>Let&apos;s start by setting cert-manager in your Kubernetes cluster. We&apos;ll use Helm for installation. Cert-manager website includes alternate <a href="https://cert-manager.io/docs/installation/?ref=blog.realvarez.com">installation methods</a> if Helm doesn&apos;t work for you. &#xA0;If you use Amazon EKS, there are EKS Blueprints to install cert-manager and cert-manager CSI driver (<a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/add-ons/cert-manager/?ref=blog.realvarez.com">Terraform</a>, <a href="https://aws-quickstart.github.io/cdk-eks-blueprints/addons/cert-manager/?ref=blog.realvarez.com">CDK</a>) .</p><p>Use Helm to install cert-manager and cert-manager CSI driver:</p><pre><code>helm repo add jetstack https://charts.jetstack.io --force-update

helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.12.0 \
  --set installCRDs=true

helm upgrade -i \
  cert-manager-csi-driver \
  jetstack/cert-manager-csi-driver \
  --namespace cert-manager \
  --wait
</code></pre><p>You should now have the following pods running in the <code>cert-manager</code> namespace:</p><pre><code>$ kubectl get pods -n cert-manager
NAME                                      READY   STATUS    RESTARTS      AGE
cert-manager-559b5d5b7d-gr8rt             1/1     Running   0             1h
cert-manager-cainjector-f5c6565d4-z8hzg   1/1     Running   0             1h
cert-manager-csi-driver-c8jmg             3/3     Running   0             1h
cert-manager-csi-driver-vkq5d             3/3     Running   0             1h
cert-manager-webhook-5f44bc85f4-ghqkm     1/1     Running   0             1h
</code></pre><p>Cert-manager-cainjector is a component of cert manager that injects certificate-related data into pods running in the cluster.It injects certificates, private keys, and other cryptographic material into application workloads, making them readily available to secure communication within the cluster.<br>By injecting certificates directly into the pods, cert-manager-cainjector eliminates the need for manual configuration or mounting of certificates. &#xA0;It simplifies securing communication between services within the Kubernetes cluster. Cert-manager-cainjector automatically updates and rotates certificates according to the defined policy</p><h3 id="create-a-private-certificate-authority-ca">Create a Private Certificate Authority (CA)</h3><p>Next, we&apos;ll create private CA, which cert-manager will use to issue and manage certificates for services that run in your cluster.</p><p>Cert-manager has two issuers that create client and CA certificates: <strong>ClusterIssuer</strong> and <strong>Issuer</strong>. The fundamental difference between ClusterIssuer and Issuer is their scope. Issuer is limited to a specific namespace, while ClusterIssuer has a cluster-wide scope.</p><p>An Issuer can only create certificates within the same namespace where it is defined. A ClusterIssuer can issue certificates across multiple namespaces in a Kubernetes cluster. For example, if you have a web application deployed in the &quot;webapp&quot; namespace and you want to issue certificates for that specific namespace, you can define an Issuer resource within the &quot;webapp&quot; namespace. If you have a cluster-wide API service that needs to communicate securely with services in other namespaces, you can define a ClusterIssuer to issue certificates for all those namespaces.</p><p>I&apos;ll create a ClusterIssuer to issue a root certificate and an Issuer to create service certificates. The Issuer resource is created in <code>sandbox</code> namespace where I&apos;ll create a TLS server later in the post. If I create more namespaces, I can create Issuers in those namespaces and use the ClusterIssuer to issue certificates using the private CA.</p><p>Create a ClusterIssuer, a root certificate, and an Issuer:</p><pre><code class="language-yaml">cat &lt;&lt; EOF &gt; cert-manager-resources.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: sandbox
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: my-selfsigned-ca
  namespace: sandbox
spec:
  isCA: true
  commonName: my-selfsigned-ca
  secretName: root-secret
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: my-ca-issuer
  namespace: sandbox
spec:
  ca:
    secretName: root-secret
EOF

kubectl apply -f cert-manager-resources.yaml
</code></pre><h3 id="create-sample-workloads-to-test-encryption">Create sample workloads to test encryption</h3><p>With cert-manager setup, let&apos;s create a TLS server. The key thing to note about service certificates is that we need to specify the <strong>Common Name</strong> (also known as CN) when creating a certificate. The CN represents the DNS name of the service the certificate is protecting.</p><p>Say, you&apos;re hosting your service at service.example.com, then you&apos;d have to specify service.example.com as the CN. If a client tries to access this service using an alternate name (e.g., service.subdomain.example.com) it will get an SSL error.</p><p>In a Kubernetes cluster, a service can have multiple DNS names. For example, a service called <code>my-simple-service</code>, can also be called <code>my-simple-service.my-namespace.svc</code>, <code>my-simple-service.my-namespace.svc.cluster.local</code>, or example.com/mysimpleservice depending on how clients access the service. So, when generating a certificate for a Kubernetes service, we need to ensure that the certificate is also valid for all alternate DNS names.</p><p>When generating a certificate for a service that&apos;s accessible at multiple DNS names, you&apos;ll have to include all alternate DNS names in the certificate. This type of certificate is called a <strong>Subject Alternate Name</strong> (or SAN) certificate as it allows multiple hostnames to be protected by a single certificate.</p><p>If you were to manually generate a certificate with alternate names, the <a href="https://gist.github.com/KeithYeh/bb07cadd23645a6a62509b1ec8986bbc?ref=blog.realvarez.com">process is pretty tedious</a>. &#xA0;Cert-manager simplifies this process. You can create a Certificate resource with alternate names like this:</p><pre><code>apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: sample-certificate
spec:
  secretName: sample-certificate-secret
  issuerRef:
    name: private-ca-issuer
    kind: Issuer
  commonName: example.com
  dnsNames:
    - example.com
    - www.example.com
  ipAddresses:
    - 192.168.0.1
</code></pre><p>When you create a certificate, cert-manager creates a Kubernetes secret in the same namespace. You can then mount this secret in your pods.</p><p>But, there&apos;s an even easier way to get certificates for your applications. This is where <a href="https://cert-manager.io/docs/usage/certificate/?ref=blog.realvarez.com">cert-manager CSI driver</a> comes in. You can use cert-manager CSI driver to create service certificates on the fly. You can define certificates request right in your pod template. &#xA0;It runs as daemonset and creates certificates without creating a secret. Your keys are stored on the node and they never leave the node.</p><p>Here are benefits of using cert-manager CSI driver:</p><ul><li>Ensure private keys never leave the node and are never sent over the network. All private keys are stored locally on the node.</li><li>Unique key and certificate per application replica with a grantee to be present on application run time.</li><li>Reduce resource management overhead by defining certificate request spec in-line of the Kubernetes Pod template.</li><li>Automatic renewal of certificates based on expiry of each individual certificate.</li><li>Keys and certificates are destroyed during application termination.</li><li>Scope for extending plugin behavior with visibility on each replica&apos;s certificate request and termination.</li></ul><p>Create a sample web service:</p><pre><code>cat &lt;&lt; EOF &gt; https-server-using-csi-driver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: sample-api
  name: sample-api
  namespace: sandbox
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sample-api
  template:
    metadata:
      labels:
        app: sample-api
    spec:
      restartPolicy: Always
      volumes:
      - name: tls
        csi:
          driver: csi.cert-manager.io
          readOnly: true
          volumeAttributes:
             csi.cert-manager.io/issuer-name: my-ca-issuer
             csi.cert-manager.io/dns-names: \${POD_NAME}.\${POD_NAMESPACE}.svc.cluster.local,sample-api-service.sandbox.svc.cluster.local,sample-api-service.sandbox.svc.,sample-api-service.
      containers:
      - name: sample-api
        image: public.ecr.aws/k9t3d5o9/secureservice:latest
        volumeMounts:
        - mountPath: &quot;/etc/tls&quot;
          name: tls
          readOnly: true
        ports:
        - containerPort: 8443
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: sample-api-service
  name: sample-api-service
  namespace: sandbox
spec:
  ports:
  - port: 443
    protocol: TCP
    targetPort: 8443
  selector:
    app: sample-api
  type: ClusterIP
EOF

kubectl apply -f https-server-using-csi-driver.yaml
</code></pre><p>In the template, I&apos;ve created a volume of type <code>csi.driver = csi.cert-manager.io</code>. I&apos;ve also specified the Issuer <code>csi.cert-manager.io/issuer-name: my-ca-issuer</code>, which is the Issuer I created earlier in the post. Finally, I&apos;ve also added all the DNS names at which my service is accessible using <code>csi.cert-manager.io/dns-names: \${POD_NAME}.\${POD_NAMESPACE}.svc.cluster.local,sample-api-service.sandbox.svc.cluster.local,sample-api-service.sandbox.svc.,sample-api-service</code>.</p><p>The container image runs a simple Python HTTPS server. The code uses a PEM file to load certificates. I use the <code>tls.crt</code> and <code>tls.key</code> files that the CSI driver creates to generate the PEM file.</p><pre><code>cat /etc/tls/tls.crt /etc/tls/tls.key &gt; tls.pem
</code></pre><p>Here&apos;s the code for the HTTPS server:</p><pre><code class="language-python">import ssl
from http.server import HTTPServer, BaseHTTPRequestHandler

class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header(&apos;Content-type&apos;, &apos;text/html&apos;)
        self.end_headers()
        self.wfile.write(b&apos;&lt;h1&gt;Serving page securely&lt;/h1&gt;&apos;)

def run(server_class, handler_class, pem_file, port):
    server_address = (&apos;0.0.0.0&apos;, port)
    httpd = server_class(server_address, handler_class)
    httpd.socket = ssl.wrap_socket(httpd.socket, certfile=pem_file, server_side=True)
    print(f&apos;Starting server on port {port}...&apos;)
    httpd.serve_forever()

if __name__ == &apos;__main__&apos;:
    pem_file = &apos;tls.pem&apos;
    port = 8443
    run(HTTPServer, SimpleHTTPRequestHandler, pem_file, port)
</code></pre><h3 id="create-a-client">Create a client</h3><p>Next we want to make sure that the certificate works. If I access this service with a web browser, I&apos;ll get an error stating the certificate is invalid. That&apos;s because it is a self-signed certificate. My browser rightfully doesn&apos;t trust the CA I just created. But if I import the root certificate, then I&apos;ll be able to access the service securely without an invalid certificate error.</p><p>Let&apos;s create a client application and we&apos;ll use <code>curl</code> to access the service:</p><pre><code>cat &lt;&lt; EOF &gt; client-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: sample-api-client
  name: sample-api-client
  namespace: sandbox
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sample-api-client
  template:
    metadata:
      labels:
        app: sample-api-client
    spec:
      volumes:
      - name: tls
        csi:
          driver: csi.cert-manager.io
          readOnly: true
          volumeAttributes:
             csi.cert-manager.io/issuer-name: my-ca-issuer
      containers:
      - name: sample-api
        image: alpine/curl
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
        args: [&quot;while true; do sleep 30; done&quot;]
        volumeMounts:
        - mountPath: &quot;/etc/tls&quot;
          name: tls
          readOnly: true
EOF

kubectl apply -f client-app.yaml
</code></pre><p>Note that since this is a client application, I am not specifying any dns-names.</p><p>Now let&apos;s test connecting from the client to the service:</p><pre><code>kubectl -n sandbox get pods -l app=sample-api-client -o name \
  | xargs -I{} kubectl -n sandbox exec {} -- \
  curl -s https://sample-api-service
</code></pre><p>I get an error:</p><pre><code>curl: (60) SSL certificate problem: unable to get local issuer certificate

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
command terminated with exit code 60
</code></pre><p>This is because I have to specify the root certificate (as this is a self-signed certificate). Let&apos;s specify the root certificate this time using curl&apos;s <code>--cacert</code> option.</p><pre><code>kubectl -n sandbox get pods -l app=sample-api-client -o name \
  | xargs -I{} kubectl -n sandbox exec {} -- \
  curl -s https://sample-api-service --cacert /etc/tls/ca.crt
</code></pre><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/06/28A6FEAD-EEF3-42AA-B781-37B5CF2637C5.png" class="kg-image" alt="Cert-Manager: Encrypt Kubernetes service to service communication with self-signed certificates" loading="lazy" width="1832" height="192"></figure><p>The service is accessible at <a href="https://sample-api-service/?ref=blog.realvarez.com">https://sample-api-service</a>. Now let&apos;s verify the certificate also works when we access the service using <code>sample-api-service.sandbox.svc.cluster.local</code> and <code>sample-api-service.sandbox.svc.</code> names.</p><pre><code>kubectl -n sandbox get pods -l app=sample-api-client -o name \
  | xargs -I{} kubectl -n sandbox exec {} -- \
  curl -s https://sample-api-service.sandbox.svc --cacert /etc/tls/ca.crt

kubectl -n sandbox get pods -l app=sample-api-client -o name \
  | xargs -I{} kubectl -n sandbox exec {} -- \
  curl -s https://sample-api-service.sandbox.svc.cluster.local --cacert /etc/tls/ca.crt
</code></pre><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/06/F3DE8CF1-1D4D-45FC-854A-4C0342CE7247.png" class="kg-image" alt="Cert-Manager: Encrypt Kubernetes service to service communication with self-signed certificates" loading="lazy" width="1852" height="326"></figure><p>The service is accessible at all 3 addresses.</p><p>There you have it. The communication from client to server now uses TLS encryption.</p><h2 id="cleanup">Cleanup</h2><p>Delete the resources created in this post:</p><pre><code>kubectl -n sandbox delete deployments.apps sample-api sample-api-client
</code></pre><p>If you&apos;d like to uninstall cert-manager and cert-manager-csi-driver:</p><pre><code>helm uninstall -n cert-manager cert-manager-csi-driver 
helm uninstall -n cert-manager cert-manager
</code></pre><h2 id="conclusion">Conclusion</h2><p>In this post I showed you how to encrypt traffic in a Kubernetes cluster using cert-manager. I created a private CA and used it to generate service and client certificates. I also showed you can use cert-manager CSI driver to easily issue certificates for your applications.</p><p>If your applications need additional certificates, you can use cert-manager <a href="https://cert-manager.io/docs/projects/trust-manager/?ref=blog.realvarez.com">trust-manager</a> to inject trust bundles into your Kubernetes pods.</p>]]></content:encoded></item><item><title><![CDATA[Run more pods per GPU with NVIDIA Multi-Instance GPU]]></title><description><![CDATA[This post explains how to maximize GPU resource usage in Amazon EKS clusters. ]]></description><link>https://realvz.github.io/blog/get-more-out-of-gpus-with-nvidia-multi-instance-gpu/</link><guid isPermaLink="false">646d49edf582930001a235ee</guid><category><![CDATA[NVIDIA]]></category><category><![CDATA[Machine Learning]]></category><category><![CDATA[Amazon EKS]]></category><category><![CDATA[Kubernetes]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Tue, 23 May 2023 23:35:20 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1536620752150-a7e9e62a62ee?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI5fHx2ZXJ0aWNhbCUyMGxpbmVzfGVufDB8fHx8MTY4NDg4NDU5OHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1536620752150-a7e9e62a62ee?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI5fHx2ZXJ0aWNhbCUyMGxpbmVzfGVufDB8fHx8MTY4NDg4NDU5OHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU"><p>Machine learning (ML) workloads require tremendous amounts of computing power. Of all the infrastructure components that scalable ML applications require, GPUs are the most critical. GPUs, with their parallel processing capabilities, have revolutionized domains like deep learning, scientific simulations, and high-performance computing. But not all ML workloads require the same amount of resources. Traditionally, ML scientists have had to pay for a full GPU regardless of whether they needed it.</p><p>In 2020, NVIDIA introduced Multi-Instance GPU (MIG) sharing. This feature partitions a GPU into multiple, smaller, fully isolated GPU instances. It is particularly beneficial for workloads that do not fully saturate the GPU&#x2019;s compute capacity. It allows users to run multiple workloads in parallel on a single GPU to maximize resource utilization. This post shows how to use MIG on Amazon EKS.</p><h2 id="nvidia-multi-instance-gpu">NVIDIA Multi-Instance GPU</h2><p>MIG is a feature of NVIDIA GPUs based on NVIDIA Ampere architecture. It allows you to maximize the value of NVIDIA GPUs and reduce resource wastage. Using MIG, you can partition a GPU into smaller GPU instances, called MIG devices. Each MIG device is fully isolated with its own high-bandwidth memory, cache, and compute cores. You can create <em>slices</em> to control the amount of memory and number of compute resources per MIG device.</p><p>MIG gives you the ability to fine tune the amount of GPU resources your workloads get. This feature provides guaranteed quality of service (QoS) with deterministic latency and throughput to ensure workloads can safely share GPU resources without interference.</p><p>NVIDIA has extensive <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/?ref=blog.realvarez.com">documentation explaining the inner workings of MIG</a>, so I won&#x2019;t repeat the information here.</p><h2 id="using-mig-with-kubernetes">Using MIG with Kubernetes</h2><p>Many customers I work with choose Kubernetes to operate their ML workloads. Kubernetes provides a powerful and scalable scheduling mechanism, making it easier to orchestrate workloads on a cluster of virtual machines. Kubernetes also has a vibrant community building tools like <a href="https://www.kubeflow.org/?ref=blog.realvarez.com">Kubeflow</a> that make it easier to build, deploy, and manage ML pipelines.</p><p>MIG on Kubernetes is still an underutilized feature due its complexity. NVIDIA documentation is partly to be blamed here. While NVIDIA&apos;s documentation explains how MIG works extensively (albeit with a lot of repetition), it is lacking when it comes to providing resources like tutorials and example for MIG deployments and configurations on Kubernetes. What makes matters worse is that to use MIG on Kubernetes, you have to install a bunch of resources such as the NVIDIA driver, NVIDIA container runtime, and device plugins.</p><p>Thankfully, <a href="https://github.com/NVIDIA/gpu-operator?ref=blog.realvarez.com">NVIDIA GPU Operator</a> automates the deployment, configuration, and monitoring GPU resources in Kubernetes. It simplifies installing the components necessary for using MIG on Kubernetes. Its key features are:</p><ul><li>Automatic GPU driver installation and management</li><li>Automatic GPU resource allocation and scheduling</li><li>Automatic GPU monitoring and alerting</li><li>Support for NVIDIA Container Runtime</li><li>Support for NVIDIA Multi-Instance GPU (MIG)</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/05/image.png" class="kg-image" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU" loading="lazy" width="2392" height="650"><figcaption>NVIDIA GPU Operator</figcaption></figure><p>The operator installs the following components:</p><ul><li><strong>NVIDIA device driver</strong></li><li><a href="https://github.com/kubernetes-sigs/node-feature-discovery?ref=blog.realvarez.com"><strong>Node Feature Discovery</strong></a>. Detects hardware features on the node</li><li><a href="https://github.com/NVIDIA/gpu-feature-discovery?ref=blog.realvarez.com"><strong>GPU Feature Discovery</strong></a>. Automatically generates labels for the set of GPUs available on a node</li><li><a href="https://github.com/NVIDIA/dcgm-exporter?ref=blog.realvarez.com"><strong>NVIDIA DCGM</strong></a> Exporter. Exposes GPU metrics exporter for <a href="https://prometheus.io/?ref=blog.realvarez.com">Prometheus</a> leveraging <a href="https://developer.nvidia.com/dcgm?ref=blog.realvarez.com">NVIDIA DCGM</a></li><li><a href="https://github.com/NVIDIA/k8s-device-plugin?ref=blog.realvarez.com"><strong>Device Plugin</strong></a>. Exposes the number of GPUs on each nodes of your cluster, keeps track of the health of your GPUs, and runs GPU enabled containers in your Kubernetes cluster</li><li><strong>Device Plugin Validator</strong>. Runs a series of validations via <code>InitContainers</code> for each component and writes out results under <code>/run/nvidia/validations</code></li><li><a href="https://github.com/NVIDIA/nvidia-container-toolkit?ref=blog.realvarez.com"><strong>NVIDIA Container Toolkit</strong></a></li><li><strong>NVIDIA CUDA Validator</strong></li><li><strong>NVIDIA Operator Validator</strong>.Validates driver, toolkit, CDA, and NVIDIA Device Plugin</li><li><a href="https://github.com/NVIDIA/mig-parted?ref=blog.realvarez.com"><strong>NVIDIA MIG Manager</strong></a>. <a href="https://github.com/NVIDIA/mig-parted/tree/main?ref=blog.realvarez.com">MIG Partition Editor</a> for NVIDIA GPUs in Kubernetes clusters</li></ul><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/05/image-5.png" class="kg-image" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU" loading="lazy" width="1950" height="1170"></figure><h2 id="nvidia-gpu-operator-on-amazon-eks">NVIDIA GPU Operator on Amazon EKS</h2><p>While NVIDIA GPU Operator makes it easy to use GPUs in Kubernetes, some of its components require newer versions of the Linux kernel and operating system. Amazon EKS provides a Linux AMI for GPU workloads that pre-installs NVIDIA drivers and container runtime. At the time of writing, this AMI provides Linux kernel 5.4. However, NVIDIA GPU Operator Helm Charts default are configured for Ubuntu or Centos 8. Therefore, making NVIDIA GPU Operator work on Amazon EKS is not as simple as executing:</p><pre><code>helm install gpu-operator nvidia/gpu-operator
</code></pre><h2 id="walkthrough">Walkthrough</h2><p>Let&#x2019;s start the walkthrough by installing NVIDIA GPU Operator. You&#x2019;d need an EKS cluster with a node group made up of EC2 instances that come with NVIDIA GPUs (P4, P3, and G4 instances). Here&#x2019;s an <a href="https://eksctl.io/?ref=blog.realvarez.com">eksctl</a> manifest if you&#x2019;d like to create a new cluster for this walkthrough:</p><pre><code>apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: p4d-cluster
  region: eu-west-1
managedNodeGroups:
  - name: demo-gpu-workers
    instanceType: p4d.24xlarge
    minSize: 1
    desiredCapacity: 1
    maxSize: 1
    volumeSize: 200
</code></pre><p>I am going to use a P4d.24XL instance for this demo. Each P4d.24XL EC2 instance has 8 NVIDIA A100 Tensor core GPUs. Each A100 GPU has 40GB memory. By default, you can only run one GPU workload per GPU with each pod getting a 40GB GPU memory slice. This means you are limited to running 8 pods per instance.</p><p><br>Using MIG, you can partition each GPU to run multiple pods per GPU. On a P4d.24XL node with 8 A100 GPUs, you can create 7 5GB A100 slices per GPU. As a result, you can run 7*8 = 56 pods concurrently. Alternatively, you can create 24 pods with 10GB slices, or 16 pods with 20GB slices, or 8 pods with 20GB slices.</p><p><br>Since the latest versions of the components that the operator installs are incompatible with the current version of Amazon EKS optimized accelerated Amazon Linux AMI, I have manually set the versions of incompatible components to a version that works with the AMI.</p><p><br>Install NVIDIA GPU Operator:</p><pre><code class="language-bash">helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
   &amp;&amp; helm repo update

helm upgrade gpuo \
    nvidia/gpu-operator \
    --set driver.enabled=true \
    --set mig.strategy=mixed \
    --set devicePlugin.enabled=true \
    --set migManager.enabled=true \
    --set migManager.WITH_REBOOT=true \
    --set toolkit.version=v1.13.1-centos7 \
    --set operator.defaultRuntime=containerd \
    --set gfd.version=v0.8.0 \
    --set devicePlugin.version=v0.13.0 \
    --set migManager.default=all-balanced
</code></pre><p>View the resources created by GPU Operator:</p><pre><code class="language-bash">$ kubectl get pods
NAME                                                  READY   STATUS      RESTARTS   AGE
gpu-feature-discovery-529vf                           1/1     Running     0          20m
gpu-operator-9558bc48-z4wlh                           1/1     Running     0          3d20h
gpuo-node-feature-discovery-master-7f8995bd8b-d6jdj   1/1     Running     0          3d20h
gpuo-node-feature-discovery-worker-wbtxc              1/1     Running     0          20m
nvidia-container-toolkit-daemonset-lmpz8              1/1     Running     0          20m
nvidia-cuda-validator-bxmhj                           0/1     Completed   1          19m
nvidia-dcgm-exporter-v8p8f                            1/1     Running     0          20m
nvidia-device-plugin-daemonset-7ftt4                  1/1     Running     0          20m
nvidia-device-plugin-validator-pf6kk                  0/1     Completed   0          18m
nvidia-mig-manager-82772                              1/1     Running     0          18m
nvidia-operator-validator-5fh59                       1/1     Running     0          20m
</code></pre><p>GPU Feature Discovery adds labels to the node that help Kubernetes schedule workloads that require a GPU. You can see the label by describing the node:</p><pre><code>$ kubectl describe node 
...
Allocatable:
  attachable-volumes-aws-ebs:  39
  cpu:                         95690m
  ephemeral-storage:           18242267924
  hugepages-1Gi:               0
  hugepages-2Mi:               0
  memory:                      1167644256Ki
  nvidia.com/gpu:              8
  pods:                        250
...
</code></pre><p>Pods can request a GPU by specifying GPU in resources. Here&apos;s a sample pod manifest:</p><pre><code>kind: Pod
metadata:
  name: dcgmproftester-1
spec:
  restartPolicy: &quot;Never&quot;
  containers:
  - name: dcgmproftester11
    image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04
    args: [&quot;--no-dcgm-validation&quot;, &quot;-t 1004&quot;, &quot;-d 30&quot;]  
    resources:
      limits:
         nvidia.com/gpu: 1    
    securityContext:
      capabilities:
        add: [&quot;SYS_ADMIN&quot;]  

</code></pre><p>We won&apos;t create a pod that uses a full GPU because that will work out of the box. Instead, we&apos;ll create pods that use partial GPUs.</p><h3 id="creating-mig-partitions-on-kubernetes">Creating MIG partitions on Kubernetes</h3><p>NVIDIA provides two strategies for exposing MIG partitioned devices on a Kubernetes node. In <strong>single strategy</strong>, a node only exposes a single type of MIG devices across all GPUs. Whereas, <strong>Mixed strategy</strong> allows you to create multiple different sized MIG devices across all of a node&apos;s GPUs.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/05/image-1.png" class="kg-image" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU" loading="lazy" width="960" height="175"><figcaption>MIG device naming</figcaption></figure><p>Using MIG single strategy, you can create similar sized MIG devices. On a P4d.24XL, you can create &#xA0;56 1g.5gb slices, or 24 2g.10gb slices, or 16 3g.20gb slices, or a 1 4g.40gb or 7g.40gb slice.</p><p>Mixed strategy will allow you to create a few 1g.5gb along with a few 2g.10gb and 3g.20gb slices. It is useful when your cluster has workloads with varying GPU resource requirements.</p><h3 id="create-mig-devices-with-single-strategy">Create MIG devices with single strategy</h3><p>Let&apos;s create a single strategy and see how to use it with Kubernetes. NVIDIA GPU Operator makes it easy to create MIG partitions. To configure partitions, all you have to do is label the node. MIG manager runs as daemonset on all nodes. When it detects node labels, it will use <a href="https://github.com/NVIDIA/mig-parted?ref=blog.realvarez.com"><code>mig-parted</code></a> to create MIG devices.</p><p>Label a node to create 1g.5gb MIG devices across all GPUs (replace <code>$NODE</code> with a node in your cluster):</p><pre><code>kubectl label nodes $NODE nvidia.com/mig.config=all-1g.5gb --overwrite
</code></pre><p>Two things will happen once you label the node this way. First, the node will no longer advertise any full GPUs and the <code>nvidia.com/gpu</code> label will be set to 0. Second, &#xA0;your node will advertise 56 1g.5gb MIG devices.</p><pre><code>$ kubectl describe node $NODE
...
  nvidia.com/gpu:              0
  nvidia.com/mig-1g.5gb:       56
...
</code></pre><p><em>Please note that it may take a few seconds for the change to take effect. The node will have a label <code>nvidia.com/mig.config.state=pending</code> when the change is still in progress. Once MIG manager completes partitioning, the label will be set to <code>nvidia.com/mig.config.state=success</code></em>.</p><p><br>We can now create a deployment that uses MIG devices.</p><p><br>Create a deployment:</p><pre><code>cat &lt;&lt; EOF &gt; mig-1g-5gb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mig1.5
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mig1-5
  template:
    metadata:
      labels:
        app: mig1-5
    spec:
      containers:
      - name: vectoradd
        image: nvidia/cuda:8.0-runtime
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
        args: [&quot;nvidia-smi &amp;&amp; tail -f /dev/null&quot;]
        resources:
          limits:
            nvidia.com/mig-1g.5gb: 1
EOF
</code></pre><p>You should now have a pod running that consumes 1x 1g.5gb MIG device.</p><pre><code>$ kubectl get deployments.apps mig1.5
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
mig1.5   1/1     1            1           1h
</code></pre><p>Let&apos;s scale the deployment to 100 replicas. Only 56 pods will get created because the node can only accommodate 56 1g.5gb MIG devices (8 GPUs * 7 MIG slices per GPU) .</p><p>Scale the deployment:</p><pre><code>kubectl scale deployment mig1.5 --replicas=100
</code></pre><p>Notice that only 56 pods become available:</p><pre><code>kubectl get deployments.apps mig1.5
NAME     READY    UP-TO-DATE   AVAILABLE   AGE
mig1.5   56/100   100          56          1h
</code></pre><p>Exec into one of the containers and run <code>nvidia-smi</code> to view allocated GPU resources.</p><pre><code>kubectl exec &lt;YOUR MIG1.5 POD&gt; -ti -- nvidia-smi
</code></pre><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/05/image-2.png" class="kg-image" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU" loading="lazy" width="1920" height="1170"></figure><p>As you can see, this pod only has 5gb memory.</p><p>Let&apos;s scale the deployment down to 0:</p><pre><code>kubectl scale deployment mig1.5 --replicas=0
</code></pre><h3 id="create-mig-devices-with-mixed-strategy">Create MIG devices with mixed strategy</h3><p>In single strategy, all MIG devices were 1g.5gb devices. Now let&apos;s slice the GPUs so that each node supports multiple MIG device configurations. MIG manager uses a configmap to store MIG configuration. When we labeled the node with <code>all-1g.5gb</code>, MIG partition editor uses the configmap to determine the partition scheme.</p><pre><code>$ kubectl describe configmaps default-mig-parted-config
...

  all-1g.5gb:
    - devices: all
      mig-enabled: true
      mig-devices:
        &quot;1g.5gb&quot;: 7

...
</code></pre><p>This configmap also includes other profiles like <code>all-balanced</code>. The <code>all-balanced</code> profile creates 2x 1g.10gb, 1x 2g.20gb, and 1x 3g.40gb MIG devices per GPU. &#xA0;You can create your own custom profile by editing the configmap.</p><p><code>all-balanced</code> MIG profile:</p><pre><code>$ kubectl describe configmaps default-mig-parted-config

...
  all-balanced:
    - device-filter: [&quot;0x20B010DE&quot;, &quot;0x20B110DE&quot;, &quot;0x20F110DE&quot;, &quot;0x20F610DE&quot;]
      devices: all
      mig-enabled: true
      mig-devices:
        &quot;1g.5gb&quot;: 2
        &quot;2g.10gb&quot;: 1
        &quot;3g.20gb&quot;: 1
...
</code></pre><p>Let&apos;s label the node to use <code>all-balanced</code> MIG profile:</p><pre><code>kubectl label nodes $NODE nvidia.com/mig.config=all-balanced --overwrite
</code></pre><p>Once the node has <code>nvidia.com/mig.config.state=success</code> label, describe the node and you&apos;ll see multiple MIG devices listed in the node:</p><pre><code>$ kubectl describe node $NODE

...

  nvidia.com/mig-1g.5gb:       16
  nvidia.com/mig-2g.10gb:      8
  nvidia.com/mig-3g.20gb:      8

...
</code></pre><p>With <code>all-balanced</code> profile, this P4d.24XL node can run 16x 1g.5gb, 8x 2g.20gb, and 8x 3g.20gb pods.</p><p>Let&apos;s test this out by creating two additional deployments. One with pods that use one 2g.10gb MIG device and another using 3g.10gb MIG device.</p><p>Create deployments:</p><pre><code>cat &lt;&lt; EOF &gt; mig-2g-10gb-and-3g.20gb-deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mig2-10
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mig2-10
  template:
    metadata:
      labels:
        app: mig2-10
    spec:
      containers:
      - name: vectoradd
        image: nvidia/cuda:8.0-runtime
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
        args: [&quot;nvidia-smi &amp;&amp; tail -f /dev/null&quot;]
        resources:
          limits:
            nvidia.com/mig-2g.10gb: 1
---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mig3-20
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mig3-20
  template:
    metadata:
      labels:
        app: mig3-20
    spec:
      containers:
      - name: vectoradd
        image: nvidia/cuda:8.0-runtime
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
        args: [&quot;nvidia-smi &amp;&amp; tail -f /dev/null&quot;]
        resources:
          limits:
            nvidia.com/mig-3g.20gb: 1
EOF
</code></pre><p>Once pods from these deployments are running, scale all three deployments to 20 replicas:</p><pre><code>kubectl scale deployments mig1.5 mig2-10 mig3-20 --replicas=20
</code></pre><p>Let&apos;s see how many of these replicas start running:</p><pre><code>kubectl get deployments
</code></pre><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/05/image-3.png" class="kg-image" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU" loading="lazy" width="1876" height="212"></figure><p>Let&apos;s see how much GPU memory a 3g.20gb pod receives:</p><pre><code>kubectl exec mig3-20-&lt;pod-id&gt; -ti -- nvidia-smi
</code></pre><p>As expected, this pod has 20GB GPU memory allocated.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2023/05/image-4.png" class="kg-image" alt="Run more pods per GPU with NVIDIA Multi-Instance GPU" loading="lazy" width="1950" height="1170"><figcaption>GPU resources in a pod with 3g.20gb MIG device</figcaption></figure><h3 id="cleanup">Cleanup</h3><p>Delete the cluster and the node group:</p><pre><code>eksctl delete cluster &lt;CLUSTER_NAME&gt;
</code></pre><h2 id="conclusion">Conclusion</h2><p>This post shows how to partition GPUs using NVIDIA Multi-Instance GPU and using it with Amazon EKS. Using MIG on Kubernetes can be complex, but NVIDIA GPU Operator simplifies the process of installing MIG dependencies and partitioning.</p><p>By leveraging the capabilities of MIG and the automation provided by the NVIDIA GPU Operator, ML scientists can optimize their GPU usage, run more workloads per GPU, and achieve better resource utilization in their scalable ML applications. With the ability to run multiple applications per GPU and tailor the allocation of resources, you can optimize your ML workloads to achieve higher scalability and performance in your applications.</p><h2 id="resources">Resources</h2><ul><li><a href="https://docs.nvidia.com/datacenter/cloud-native/mig/mig.html?ref=blog.realvarez.com">Multi-Instance GPU &#x2014; NVIDIA Cloud Native Technologies documentation</a></li><li><a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/?ref=blog.realvarez.com">NVIDIA Multi-Instance GPU User Guide :: NVIDIA Tesla Documentation</a></li><li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html?ref=blog.realvarez.com">NVIDIA GPU Operator Overview &#x2014; NVIDIA Cloud Native Technologies documentation</a></li><li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-operator-mig.html?ref=blog.realvarez.com">GPU Operator with MIG &#x2014; NVIDIA Cloud Native Technologies documentation</a></li><li><a href="https://docs.google.com/document/d/1mdgMQ8g7WmaI_XVVRrCvHPFPOMCm5LQD5JefgAh6N8g/edit?ref=blog.realvarez.com">[External] Supporting MIG in Kubernetes - Google Docs</a></li><li><a href="https://docs.google.com/document/d/1Dxx5MwG_GiBeKOuMNwv4QbO8OqA7XFdzn7fzzI7AQDg/edit?ref=blog.realvarez.com">[External] Challenges Supporting MIG in Kubernetes - Google Docs</a></li><li><a href="https://docs.google.com/document/d/1bshSIcWNYRZGfywgwRHa07C0qRyOYKxWYxClbeJM-WM/edit?ref=blog.realvarez.com">[External] Steps to Enable MIG Support in Kubernetes - Google Docs</a></li></ul>]]></content:encoded></item><item><title><![CDATA[Use containerd to handle k8s.gcr.io deprecation]]></title><description><![CDATA[<p>The Kubernetes community is getting ready for yet another major change. Until fall 2022, <code>k8s.gcr.io</code> container registry hosted many Kubernetes community-managed container images like Cluster Autoscaler, metrics-server, cluster-proportional-autoscaler. The &#x201C;gcr.io&#x201D; is Google Cloud Registry. In order to be vendor neutral, the community is moving away</p>]]></description><link>https://realvz.github.io/blog/use-containerd-to-handle-k8s-gcr-io-deprecation/</link><guid isPermaLink="false">642d183e001c0600010ff1d4</guid><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Wed, 05 Apr 2023 06:52:39 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1542159919831-40fb0656b45a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxzdW5zZXR8ZW58MHx8fHwxNjgwNjc3MDgx&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1542159919831-40fb0656b45a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxzdW5zZXR8ZW58MHx8fHwxNjgwNjc3MDgx&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Use containerd to handle k8s.gcr.io deprecation"><p>The Kubernetes community is getting ready for yet another major change. Until fall 2022, <code>k8s.gcr.io</code> container registry hosted many Kubernetes community-managed container images like Cluster Autoscaler, metrics-server, cluster-proportional-autoscaler. The &#x201C;gcr.io&#x201D; is Google Cloud Registry. In order to be vendor neutral, the community is moving away from using Google Cloud Registry to host container images.</p><p>As a result, starting March 20th, traffic from the old <code>k8s.gcr.io</code> registry is being redirected to <code>registry.k8s.io</code>. The older <code>k8s.gcr.io</code> will remain functioning for sometime but it is eventually getting deprecated.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/FC421666-8104-4CD9-8C7F-8E4F4BCE2B83_2/6olkWYEK1kl10toAonqYL40P1ZSQIUY2N6iHRwId78kz/Image.jpeg" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><h2 id="what%E2%80%99s-the-impact">What&#x2019;s the impact?</h2><p>The change that occurred six days after Pi day is unlikely to cause major problems. There are some edge cases. But unless you operate in an airgapped or highly restrictive environment that applies strict domain name access controls, you won&#x2019;t notice the change.</p><p>This doesn&#x2019;t mean that there&#x2019;s no action required. Now is the time to scan code repositories and clusters for usage of the old registry. Failing to act will result in cluster components failing.</p><p>Once the old registry goes away, <strong>Kubernetes will not be able to create new Pods</strong> (unless image is cached) if the container uses an image hosted on <code>k8s.gcr.io</code>.</p><h2 id="what-do-you-need-to-change">What do you need to change?</h2><p>Cluster owners and development teams have to ensure they are not using any images stored in the old registry. The change is fairly simple. It&apos;s pretty simple, really.</p><p>You need to change your manifests to use <code>registry.k8s.io</code> container registry.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/EB0981CC-3DC8-49EE-A6F5-A857A69CC949_2/D0F0mQ2JMAl55JNCuS66lUylRznVZywEMdojiRqkc0sz/Image.png" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><p>You can find out which Pods use the old registry using <code>kubectl</code>:</p><pre><code class="language-bash">kubectl get pods --all-namespaces -o jsonpath=&quot;{.items[*].spec.containers[*].image}&quot; |\0;31;37M0;31;38m
tr -s &apos;[[:space:]]&apos; &apos;\n&apos; |\
sort |\
uniq -c | grep -i gcr.io
</code></pre><p>Here are the Pods in my test cluster that use the old registry:</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/ED8BD697-7BD9-4D58-83FC-833C66606E97_2/dcAQoH7ADPaoupccwi3WFMxt1PkS5ME4zXHHyxjAmdMz/Image.jpeg" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><p>These are the at-risk Pods. I&#x2019;ll have to update the container registry used in the Pods.</p><p>When hunting for references to old registry, be sure to include containers that may not be currently running in your cluster. Don&apos;t forget to scan code repositories. </p><h2 id="what-if-i-don%E2%80%99t-control-the-workloads">What if I don&#x2019;t control the workloads?</h2><p>One of my colleagues raised an intriguing question. Is there&#x2019;s a way to handle this change at a cluster level? He had a valid concern. Many large enterprises might not be able to implement this change in time before the community sunsets <code>k8s.gcr.io</code>.</p><p>I work with many customers that manage large Kubernetes clusters, but have little control over the workloads that get deployed into the cluster. Some of these clusters are shared by hundreds of development teams. The burden is on Central Platform Engineering teams to dissipate this information to individual dev teams (who are busy writing code and not checking Kubernetes news!).</p><p>So, what can these teams do to make sure when the old registry finally croaks, they don&#x2019;t get paged for in the middle of the night for <code>ErrImagePull</code> and <code>ImagePullBackOff</code>errors?</p><p>Turns out you can use containerd to handle this redirection at node level. Let&#x2019;s find out how.</p><h2 id="using-mirrors-in-containerd">Using mirrors in containerd</h2><p>Ever since Dockerhub started rate limiting image pulls, many have opted to store images in local registries. Mirrors save network bandwidth, reduce image pull time, and don&#x2019;t rate-limit.</p><p>You can configure <code>registry.k8s.io</code> as a mirror to <code>k8s.gcr.io</code> in containerd. This configuration will <strong>automatically pull images from <code>registry.k8s.io</code></strong> whenever a Pod uses an image stored in <code>k8s.gcr.io</code>.</p><p>On your worker node, append these lines in the containerd config file at <code>/etc/containerd/config.toml</code>:</p><pre><code class="language-bash">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]
   config_path = &quot;/etc/containerd/certs.d&quot;
</code></pre><p>The final file on an Amazon EKS cluster looks like this:</p><pre><code class="language-yaml">version = 2
root = &quot;/var/lib/containerd&quot;
state = &quot;/run/containerd&quot;

[grpc]
address = &quot;/run/containerd/containerd.sock&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]
default_runtime_name = &quot;runc&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;]
sandbox_image = &quot;602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/pause:3.5&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
runtime_type = &quot;io.containerd.runc.v2&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
SystemdCgroup = true

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni]
bin_dir = &quot;/opt/cni/bin&quot;
conf_dir = &quot;/etc/cni/net.d&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]
config_path = &quot;/etc/containerd/certs.d&quot;
</code></pre><p>Next, create a directory called <code>k8s.gcr.io</code> and a <code>hosts.toml</code> file inside it:</p><pre><code class="language-bash">mkdir -p /etc/containerd/certs.d/k8s.gcr.io

cat &lt;&lt; EOF &gt; /etc/containerd/certs.d/k8s.gcr.io/hosts.toml
server = &quot;https://k8s.gcr.io&quot;

[host.&quot;https://registry.k8s.io&quot;]
capabilities = [&quot;pull&quot;, &quot;resolve&quot;]
EOF
</code></pre><p>Image pull requests to <code>k8s.gcr.io</code> will now be sent to <code>registry.k8s.io</code>.</p><p>Restart containerd and kubelet for the change to take effect.</p><pre><code class="language-bash">systemctl restart containerd kubelet
</code></pre><p>Let&#x2019;s validate that images are indeed getting pulled from the new registry. I added an entry to my <code>/etc/hosts</code> file to break <code>k8s.gcr.io</code>:</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/8AEBA792-CA42-4C8B-9C8D-E34AC55A9847_2/7A4cAKtUBNGVqmUysXRwpW0WGy3cNGz14xxujKlsmc8z/Image.png" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><p>Containerd can no longer pull an image from <code>k8s.gcr.io</code></p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/D382C5B0-437D-44E3-BAA2-E32032FF7E29_2/zmbCw5wl0LyV1TNJdiEQxCnSqKBS3s4IDZWYljKaLvkz/Image.jpeg" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><p>I can tell <code>ctr</code> to use the mirror by specifying the <code>&#x2014;hosts-dir</code> parameter:</p><pre><code class="language-bash">ctr images pull --hosts-dir &quot;/etc/containerd/certs.d&quot; k8s.gcr.io/pause:latest
</code></pre><p>This time the operation succeeds.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/94C5856D-E54D-414D-86E6-5B7C70A7989B_2/2bTbUm39iI52DAfS4D9GgH1T1LW5pBbWxTHNIjgS2v4z/Image.jpeg" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><p>Any Pods I create now onwards will use the new registry even though the manifests reference old registry. Here&#x2019;s a test using a pause container.</p><pre><code class="language-bash">kubectl create deployment pause --image k8s.gcr.io/pause
</code></pre><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/13FE78CC-1DDD-47D9-B875-EE3FB15F916E/88014DF3-E73E-4A60-B881-64C31B8210B6_2/lnuzXKg7G4RwURvQUxrZwRFuBDljKWpX6m01nVydnxoz/Image.jpeg" class="kg-image" alt="Use containerd to handle k8s.gcr.io deprecation" loading="lazy"></figure><p>Perfect! Kubernetes could create Pods even though I blocked <code>k8s.gcr.io</code> on the node.</p><h2 id="what%E2%80%99s-the-best-way-to-implement-this-in-production">What&#x2019;s the best way to implement this in production?</h2><p>In my little demo, I changed a single node in the cluster. What about the rest of the nodes?</p><p>There are three ways you can use to implement this change on every node in your cluster:</p><ol><li>The easiest way is to use a daemonset to change to containerd config.toml and add hosts.toml file. IBM cloud has shared <a href="https://raw.githubusercontent.com/IBM-Cloud/kube-samples/master/containerd-registry-daemonset-example?ref=blog.realvarez.com">this</a> on Github</li><li>You can use <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html?ref=blog.realvarez.com">EC2 user data</a> or <a href="https://aws.amazon.com/systems-manager/?ref=blog.realvarez.com">AWS Systems Manager</a> to make this change when a node gets created</li><li>You can <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-ami-build-scripts.html?ref=blog.realvarez.com">create your own AM</a>I</li></ol><h2 id="what-if-i-use-docker-as-runtime">What if I use Docker as runtime?</h2><p>Starting Kubernetes version <code>1.24</code>, containerd is the only runtime available in Amazon EKS AMIs. If you have an edge case that requires using Docker, there&apos;s still hope.</p><p>Docker also has support for registry mirrors. <a href="https://docs.docker.com/registry/recipes/mirror/?ref=blog.realvarez.com#configure-the-docker-daemon">Here&#x2019;s</a> the documentation page you need.</p><h2 id="don%E2%80%99t-rely-on-stop-gaps">Don&#x2019;t rely on stop gaps</h2><p>While the solution included in this post works, I recommend only using as a safety measure. The main reason is that you&#x2019;ll need to customize the Amazon EKS AMI or create your own AMI to use it. </p><p>You&#x2019;ll have less operational overhead if you can simply use EKS AMIs as is. The best way to handle this registry deprecation is to update manifests.</p><p>Oh, and by the way, you can also use mirrors to enforce pull through cache.</p>]]></content:encoded></item><item><title><![CDATA[Reduce Amazon EKS cost by scaling node groups to zero]]></title><description><![CDATA[<p>Amazon EKS just released the support for <a href="https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-kubernetes-version-1-24/?ref=blog.realvarez.com">Kubernetes version 1.24</a>. The new version supports a bunch of cool features. My favorite feature in this release is the ability to scale EKS managed node groups to (and from) zero.</p><p>Many customers I engage with have some workloads that don&#x2019;</p>]]></description><link>https://realvz.github.io/blog/reduce-amazon-eks-cost-by-scaling-node-groups-to-zero/</link><guid isPermaLink="false">63753fe9cfa813000134c31c</guid><category><![CDATA[Amazon EKS]]></category><category><![CDATA[Cost]]></category><category><![CDATA[Kubernetes]]></category><category><![CDATA[Scaling]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Wed, 16 Nov 2022 19:56:15 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1591617870684-6e861e6a48ad?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGdyYXBofGVufDB8fHx8MTY2ODYyODQ5Ng&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1591617870684-6e861e6a48ad?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGdyYXBofGVufDB8fHx8MTY2ODYyODQ5Ng&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Reduce Amazon EKS cost by scaling node groups to zero"><p>Amazon EKS just released the support for <a href="https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-kubernetes-version-1-24/?ref=blog.realvarez.com">Kubernetes version 1.24</a>. The new version supports a bunch of cool features. My favorite feature in this release is the ability to scale EKS managed node groups to (and from) zero.</p><p>Many customers I engage with have some workloads that don&#x2019;t run continuously. A good example is building software. Software build jobs run when new development teams push new code. Outside of business hours, the supporting infrastructure (like nodes) sits idle. Customers use autoscaling to scale down node groups, but managed node groups required a minimum of 1 node in a node group previously. That&#x2019;s one node too many, especially when you need beefier and costly nodes with GPUs.</p><p>Scaling down to zero results in significant cost savings in such cases. In my opinion, you wouldn&#x2019;t want to scale your entire cluster to zero. After all, you&#x2019;d need some nodes to run Cluster Autoscaler and other shared services like Prometheus, AWS Load Balancer, CoreDNS, etc. You can use EKS on Fargate to run some of these services. But keep in mind that Prometheus requires a block storage, and AWS Fargate doesn&#x2019;t support Amazon EBS yet.</p><p>You&#x2019;d want to run a managed node group for your shared services, like Cluster Autoscaler, that run continuously. You can then add another node group for workloads that spawn periodically, and scale that node group to zero.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/1CAE5716-0A06-487B-94D9-ED3A26E55B46/F10EE54C-3121-4A95-A3E8-2084AC07ED39_2/0ePoBv7QUOJphRVgz14AABF7CLN8q6hh0yNumolViQMz/Untitled%20Diagram.drawio.png" class="kg-image" alt="Reduce Amazon EKS cost by scaling node groups to zero" loading="lazy"></figure><h2 id="cluster-autoscaler-managed-node-group-cache">Cluster Autoscaler Managed Node group cache</h2><p>The Kubernetes Cluster Autoscaler project added support for scaling node groups to and from zero in version 0.6.1. However, it only worked if you added <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md?ref=blog.realvarez.com#how-can-i-scale-a-node-group-to-0">specific tags</a> to Auto Scaling groups. In other words, after creating a managed node group, you had to find out the associated Auto Scaling group and add Cluster Autoscaler tags yourself.</p><p>Starting Kubernetes version 1.24, you can create node groups (or tag existing node groups) with Cluster Autoscaler tags and Cluster Autoscaler will scale that node group to and from zero.</p><p>To enable scaling to and from zero, the awesome EKS team contributed a feature to the upstream Cluster Autoscaler project. The new feature adds a manage node group cache that holds labels and taints associated with managed node groups. Cluster Autoscaler now uses the EKS <a href="https://docs.aws.amazon.com/eks/latest/APIReference/API_DescribeNodegroup.html?ref=blog.realvarez.com"><code>DescribeNodegroup</code></a> API to determine a node&apos;s label and taints when there are no nodes in the node group. This allows scaling to and from zero and doesn&apos;t require adding Auto Scaling group tags.</p><h2 id="cluster-autoscaler-tags-for-scaling-to-zero">Cluster Autoscaler tags for scaling to zero</h2><p>Before you can start scaling a managed node group to and from zero, you&#x2019;d need to <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html?ref=blog.realvarez.com">add a few tags</a> to your node group. The tags you attach to your node group will help Cluster Autoscaler determine which node group to scale when a Pod is pending. You can add tags that help Cluster Autoscaler taints, labels, and node group&#x2019;s resources like <code>WindowsENI</code>, <code>PrivateIPv4Address</code>, etc.</p><p>Labels and taints will tell the Kubernetes scheduler to assign Pods to specific nodes. When those Pods don&#x2019;t have a node to run on (which will be the case when the node group is scaled to zero), Cluster Autoscaler can determine which node group to scale based on the tags. Let&#x2019;s explore it using an example.</p><h2 id="scaling-a-managed-node-group-from-zero">Scaling a managed node group from zero</h2><p>You&#x2019;d need a Kubernetes version 1.24 cluster to follow along. My EKS cluster already has node group, and I have <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html?ref=blog.realvarez.com">installed Cluster Autoscaler</a> using EKS documentation.</p><p>Cluster Autoscaler needs the permissions to call the EKS <code>DescribeNodegroup</code> API to be able to read a node group&apos;s tags. The instructions in EKS documentation currently do not add <code>DescribeNodegroup</code> permissions to the Cluster Autoscaler IAM role.</p><p>The first thing you&#x2019;d need to do is create an IAM policy that allows the Cluster Autoscaler IAM role to use <code>DescribeNodegroup</code> API.</p><p>Create a policy to allow EKS <code>DescribeNodegroup</code> &#xA0;API:</p><pre><code class="language-python">cat &gt; describe-nodegroup.json &lt;&lt; EOF
{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Action&quot;: [
                &quot;eks:DescribeNodegroup&quot;
            ],
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Resource&quot;: &quot;*&quot;
        }
    ]
}
EOF
</code></pre><p>Now you need to add this policy to the Cluster Autoscaler IAM Role. Determine the name of the IAM Role attached to the Cluster Autoscaler service account:</p><pre><code class="language-python">CA_IAM_ROLE=$(kubectl -n kube-system get  sa cluster-autoscaler -o  jsonpath=&apos;{.metadata.annotations.eks\.amazonaws\.com/role-arn}&apos; | sed &apos;s|.*/||&apos; )
</code></pre><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/1CAE5716-0A06-487B-94D9-ED3A26E55B46/6EF2F66E-E9B7-4B3D-9D75-06400FAE0BF2_2/gI025AVUilKKRF9GfkjpTeq3GmtiWypNeTVa4uE6XYYz/Image.png" class="kg-image" alt="Reduce Amazon EKS cost by scaling node groups to zero" loading="lazy"></figure><p>Then, add the policy you created to the IAM role that Cluster Autoscaler uses:</p><pre><code class="language-python">aws iam put-role-policy --role-name $CA_IAM_ROLE \
  --policy-name EKSDescribeNodegroup \
  --policy-document file://describe-nodegroup.json
</code></pre><p>Now you can start creating a new node group that you can set to scale to and from zero. Find out the role attached to an existing node group in your cluster. You can use AWS CLI to query that information.</p><pre><code class="language-python"># Store your EKS cluster name in an environment variable
EKS_CLUSTER=&lt;YOUR CLUSTER NAME&gt;
AWS_ACCOUNT=$(aws sts get-caller-identity --query &apos;Account&apos; --output text)

NODE_ROLE=$(aws eks describe-nodegroup \
  --cluster-name $EKS_CLUSTER \
  --nodegroup-name &lt;YOUR NODE GROUP NAME&gt; \
  --query &apos;nodegroup.nodeRole&apos; \
  --output text)
</code></pre><p>You&#x2019;d also need to provide the subnets for the new node group. You can use <code>describe-nodegroup</code> to find out the subnets attached to an existing node group.</p><p>Create a node group with a label that you will later use to assign Pods to nodes in this node group:</p><pre><code class="language-python">aws eks create-nodegroup \
 --cli-input-json &apos;
{
  &quot;clusterName&quot;: &quot;${EKS_CLUSTER}&quot;,
  &quot;nodegroupName&quot;: &quot;scale-to-zero&quot;,
  &quot;scalingConfig&quot;: {
     &quot;minSize&quot;: 0,
     &quot;maxSize&quot;: 5,
     &quot;desiredSize&quot;: 0
  },
  &quot;subnets&quot;: [
     &quot;&lt;subnet-ID1&gt;&quot;,
     &quot;&lt;subnet-ID2&gt;&quot;,
     &quot;&lt;subnet-ID3&gt;&quot;
   ],
  &quot;nodeRole&quot;: &quot;${NODE_ROLE}&quot;,
  &quot;labels&quot;: {
     &quot;app&quot;: &quot;frontend&quot;
  },
  &quot;tags&quot;: {
     &quot;k8s.io/cluster-autoscaler/node-template/label/app&quot;: &quot;frontend&quot;
  }
}&apos;
</code></pre><p><em>Replace the subnet IDs to match your environment.</em></p><p>Note that I added a tag <code>k8s.io/cluster-autoscaler/node-template/label/app</code> with value <code>frontend</code>. This is the same as running <code>kubectl label nodes &lt;YOUR NODE NAME&gt; app=frontend</code>. When a node gets created in this node group, it will already have label <code>app=frontend</code>.</p><p>Now that the node group is created, let&#x2019;s create a pod with a nodeSelector:</p><pre><code class="language-python">cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-test
spec:
  containers:
  - name: nginx
    image: nginx:latest
  nodeSelector:
    app: frontend
EOF
</code></pre><p>The pod will remain pending for a few minutes. In my case, the pod was in pending state for five minutes.</p><p>In the meantime, you can see Cluster Autoscaler logs:</p><p><code>kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler | grep scale-to-zero</code></p><p>Once Cluster Autoscaler adds a new node, the pod will start running. You can enable <a href="https://aws.amazon.com/blogs/containers/automatically-enable-group-metrics-collection-for-amazon-eks-managed-node-groups/?ref=blog.realvarez.com">Auto Scaling group metrics collection</a> to see how your node group scales.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/1CAE5716-0A06-487B-94D9-ED3A26E55B46/D344F40B-8692-4F2F-921C-422DA115E1B7_2/qBfnOkqluRY1yVqZBaMsB6K5xJgA1idkKOxkkfmzx8Az/Image.jpeg" class="kg-image" alt="Reduce Amazon EKS cost by scaling node groups to zero" loading="lazy"></figure><p>Great! Cluster Autoscaler saw that the test pod was pending, so it scaled the node group from 0.</p><p>Now let&#x2019;s delete the test pod and verify that the node group goes back to 0:</p><pre><code class="language-python">kubectl delete pods nginx-test
</code></pre><p>Cluster Autoscaler will notice that the node with app=frontend is not running any pods and scale down the node group (after the cooldown period).</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/1CAE5716-0A06-487B-94D9-ED3A26E55B46/EAD1C890-6DB1-455E-BABB-FAF71E5FD960_2/kDUeT7u0sJs3WqNy7SG8FBvHyHi9ALDkNEsxsuL1jtIz/Image.jpeg" class="kg-image" alt="Reduce Amazon EKS cost by scaling node groups to zero" loading="lazy"></figure><p>Perfect, the node group is back to having 0 nodes.</p><h2 id="conclusion">Conclusion</h2><p>Scaling down to zero can result in significant cost savings when you have workloads that don&#x2019;t run 24x7. With Kubernetes 1.24, all you need to do is tag your node groups with labels, taints, or resources, and Cluster Autoscaler will scale your nodes to and from zero.</p><p>Happy scaling!</p>]]></content:encoded></item><item><title><![CDATA[Book Review: The Dawn of Everything by David Graeber & David Wengrow]]></title><description><![CDATA[<p>The late David Graeber and David Wengrow are the foremost anthropologist and left-wing thinkers of our times. Their decade-long research culminates in an epic retelling of the human history. The Dawn of Everything: A New History of Humanity is an attempt to upend the premise of modern day understanding oh</p>]]></description><link>https://realvz.github.io/blog/book-review-the-dawn-of-everything-by-david-graeber-david-wengrow/</link><guid isPermaLink="false">637010b5cfa813000134c30e</guid><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Sat, 12 Nov 2022 21:32:47 GMT</pubDate><media:content url="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2022/11/41bDwt4ewZL._AC_SY780_.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/clrvv0c/2022/11/41bDwt4ewZL._AC_SY780_.jpg" alt="Book Review: The Dawn of Everything by David Graeber &amp; David Wengrow"><p>The late David Graeber and David Wengrow are the foremost anthropologist and left-wing thinkers of our times. Their decade-long research culminates in an epic retelling of the human history. The Dawn of Everything: A New History of Humanity is an attempt to upend the premise of modern day understanding oh political and social history. The book challenges the notion that private property has shaped our current social landscape. It tries to explore the origins of social inequality and define concepts of personal freedom.</p><p>Besides criticizing the works of Rousseau and Hobbes, the book challenges works by popular intellectuals like Jared Diamond, Steven Pinker, Francis Fukuyama and Yuval Noah Harari in a provocative way. These authors regurgitate the same old myths in spite of the new archeological and anthropological research results.</p><p>The authors disagree that property rights caused humankind to descend into unkind and selfish beings from an original state of egalitarian innocence. They note that hierarchy and domination, and cynical self-interest, have always been the basis of human society.</p><p>The book dispels the myths that prior to becoming capitalists, humans (hunter-gatherers) were savage, politically immature, and egalitarian. Per the authors, human societies before the advent of farming were not confined to small, egalitarian bands. In fact, hunter-gatherers traveled farther distances and mingled with other groups more than we do today.</p><blockquote>A first step towards a more accurate, and hopeful, picture of world history might be to abandon the Garden of Eden once and for all, and simply do away with the notion that for hundreds of thousands of years, everyone on earth shared the same idyllic form of social organization.</blockquote><p>The core argument of the book is extremely political and challenges the status quo. The book uses anthropological researches about Native Americans, Native Andeans, Egyptians, early Mesopotamians, Eastern Europeans to posit that:</p><ol><li>The 17th century Amerindian society was socially equal much before Europeans started questioning equality in the French society that was intrinsically hierarchical. They impressed Europeans with their eloquence and powers of reasoned argument. &#x201C;When it came to questions of personal freedom, the equality of men and women, sexual mores or popular sovereignty &#x2013; or even, for that matter, theories of depth psychology18 &#x2013; indigenous American attitudes are likely to be far closer to the reader&#x2019;s own than seventeenth-century European ones.&#x201D;</li><li>People across Americas, Europe, and Africa changed social structures as seasons changed. Banding together one season and venturing in smaller groups during another. The social and political structure in these societies was more fluid than the contemporary political arrangement.</li><li>Groups across the world <em>chose</em> not to farm. Farming is hard work and humans opted for easier sources of food. The first humans farmed on flood retreat lands, which doesn&apos;t require plowing. &#x201C;Farming, as we can now see, often started out as an economy of deprivation: you only invented it when there was nothing else to be done, which is why it tended to happen first in areas where wild resources were thinnest on the ground.&#x201D; It took 3000 years for farming to become a standard practice.</li><li>Instead of tilling, threshing, irrigating, and breaking their backs, humans spend their time feasting, dancing, maintaining gardens, practicing botany, playing sports, weaving, hunting, fishing, and gathering nuts. Farming ties farmers to their lands. Non-farmers had the freedom to move as they pleased.</li><li>Unlike in current times, when capitalism and authoritarianism are standard, hunter-gatherers had a diverse social and political setups. Throughout pre-history, there were masses that achieved egalitarianism and self-governance.</li><li>Many early cities, some with up to 25,000 citizens, had no centralized administration. Early societies had the freedom to choose the society they lived in, which is lost in today&#x2019;s age. &#x201C;An origin for &#x2018;the state&#x2019; has long been sought in such diverse places as ancient Egypt, Inca Peru and Shang China, but what we now regard as states turn out not to be a constant of history at all; not the result of a long evolutionary process that began in the Bronze Age, but rather a confluence of three political forms &#x2013; sovereignty, administration and charismatic competition &#x2013; that have different origins.&#x201D;</li></ol><p>The main takeaway for me was the human history hasn&#x2019;t progressed in a linear fashion. Capitalism and the <em>modern</em> pseudo-democracy might very well not be the pinnacle of human evolution.</p><p>This is a very interesting book even though it is plagued by digressions and sometimes excessive polemic. 3.5/5 &#x2B50;&#xFE0F;.</p>]]></content:encoded></item><item><title><![CDATA[Autoscale Kubernetes Metrics Server]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Many organizations are happy to standardize their infrastructure platform on Kubernetes. Kubernetes gives engineers a consistent platform across cloud providers and on premises. It abstracts underlying infrastructure so engineers can focus on writing code without having tight-coupling with methods for load balancing, observability, configuration, secrets management, etc.</p>
<p>I frequently speak</p>]]></description><link>https://realvz.github.io/blog/autoscale-kubernetes-metrics-server/</link><guid isPermaLink="false">63642a6798354e00013ceffc</guid><category><![CDATA[Amazon EKS]]></category><category><![CDATA[Kubernetes]]></category><category><![CDATA[Scaling]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Thu, 03 Nov 2022 20:53:59 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1616085290809-064fcf10dc4a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGV4cGFuZHxlbnwwfHx8fDE2Njc1MDg5MTk&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1616085290809-064fcf10dc4a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGV4cGFuZHxlbnwwfHx8fDE2Njc1MDg5MTk&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Autoscale Kubernetes Metrics Server"><p>Many organizations are happy to standardize their infrastructure platform on Kubernetes. Kubernetes gives engineers a consistent platform across cloud providers and on premises. It abstracts underlying infrastructure so engineers can focus on writing code without having tight-coupling with methods for load balancing, observability, configuration, secrets management, etc.</p>
<p>I frequently speak with organizations that run their entire workloads in one or two Kubernetes clusters. Effectively, they have moved their entire data centers into Kubernetes clusters.</p>
<p>Now, Kubernetes is not without its flaws. Especially when operating at scale. Once clusters go beyond hundreds of nodes, nuanced behavior starts showing up. Besides scaling the Kubernetes control plane and data plane, platform teams have to scale Kubernetes components like CoreDNS, core components, and add-ons.</p>
<p>In this post, I am going to show how to scale the metrics server add-on, so when your cluster scales the Horizontal Pod Autoscaler can reliably scale your workload.</p>
<h2 id="scaling-kubernetes-add-ons">Scaling Kubernetes add-ons</h2>
<p>Kubernetes add-ons are software packages that extend the functionality of Kubernetes. Vanilla Kubernetes clusters lack capabilities that most production clusters require. For example, data plane scaling functionality in Kubernetes is provided by the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler?ref=blog.realvarez.com">Kubernetes Cluster Autoscaler</a>. Metrics server collects Node and Pod metrics. Log aggregators like Fluent Bit and Fluentd allow you to collect Kubernetes application and system logs.</p>
<p>It is a best practice to deploy these add-ons with resource limits to account for bugs and memory leaks. Requests and limits allow us to control system resource allocated to each Pod. This feature makes it safer to run multiple Pods on a node without worrying about resource contention or oversubscription.</p>
<p>Add-ons are deployed either as DaemonSets or Deployments. As a cluster scales, DaemonSets scale automatically as they run once per node. However, add-ons deployed as Deployments do not scale automatically because they are unaware of the size of the cluster&#x2019;s scale.</p>
<p>As the cluster scales, add-ons such as <a href="https://github.com/kubernetes-sigs/metrics-server?ref=blog.realvarez.com">metrics-server</a> and <a href="https://github.com/kubernetes/kube-state-metrics?ref=blog.realvarez.com">kube-state-metrics</a> have to hold more data in memory. The default resource requests on the metrics-server are sized for clusters of up to 100 nodes. In clusters larger than that, the metrics-server can run out of memory, which breaks the Horizontal Pod Autoscaler.</p>
<p>As a result, operators have to scale add-ons, such as the metrics server, vertically as the cluster scales. <a href="https://github.com/kubernetes/autoscaler/tree/master/addon-resizer?ref=blog.realvarez.com">Addon-resizer</a> is an open source tool you can use to scale Deployments in proportion to the data plane. While the Kubernetes <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler?ref=blog.realvarez.com">Cluster Proportional Autoscaler</a> scales Deployments horizontally, the addon-resizer scales Deployments vertically.</p>
<p>Some cloud providers use addon-resizer to scale the metrics-server add-on. Amazon EKS currently doesn&apos;t automatically scale metrics-server. I am going to run addon-resizer to autoscale the metrics-server Deployment in an EKS cluster.</p>
<h2 id="addon-resizer">Addon-resizer</h2>
<p>Addon-resizer is a container that vertically scales a Deployment based on the number of nodes in your cluster. It scales Deployments linearly as the cluster&#x2019;s data plane grows and shrinks.</p>
<p>The container monitors your cluster periodically and increases or decreases the requests and limits of a Deployment in proportion to the number of nodes. Vertical scaling implies that <strong>addon-resizer will recreate the Pods</strong> with newer resource limits.</p>
<p>At the core of addon-resizer lies the *nanny *program.</p>
<pre><code>Usage of ./pod_nanny:
      --acceptance-offset=20: A number from range 0-100. The dependent&apos;s resources are rewritten when they deviate from expected by a percentage that is higher than this threshold. Can&apos;t be lower than recommendation-offset.
      --alsologtostderr[=false]: log to standard error as well as files
      --container=&quot;pod-nanny&quot;: The name of the container to watch. This defaults to the nanny itself.
      --cpu=&quot;MISSING&quot;: The base CPU resource requirement.
      --deployment=&quot;&quot;: The name of the deployment being monitored. This is required.
      --extra-cpu=&quot;0&quot;: The amount of CPU to add per node.
      --extra-memory=&quot;0Mi&quot;: The amount of memory to add per node.
      --extra-storage=&quot;0Gi&quot;: The amount of storage to add per node.
      --log-flush-frequency=5s: Maximum number of seconds between log flushes
      --log_backtrace_at=:0: when logging hits line file:N, emit a stack trace
      --log_dir=&quot;&quot;: If non-empty, write log files in this directory
      --logtostderr[=true]: log to standard error instead of files
      --memory=&quot;MISSING&quot;: The base memory resource requirement.
      --namespace=&quot;&quot;: The namespace of the ward. This defaults to the nanny pod&apos;s own namespace.
      --pod=&quot;&quot;: The name of the pod to watch. This defaults to the nanny&apos;s own pod.
      --poll-period=10000: The time, in milliseconds, to poll the dependent container.
      --recommendation-offset=10: A number from range 0-100. When the dependent&apos;s resources are rewritten, they are set to the closer end of the range defined by this percentage threshold.
      --stderrthreshold=2: logs at or above this threshold go to stderr
      --storage=&quot;MISSING&quot;: The base storage resource requirement.
      --v=0: log level for V logs
      --vmodule=: comma-separated list of pattern=N settings for file-filtered logging
</code></pre>
<p>The nanny program takes the base CPU and memory and adds extra resources per node. Here&#x2019;s the formula it uses:</p>
<pre><code>Base  CPU + (Extra CPU * Nodes)
</code></pre>
<p>Let&#x2019;s say we allocate 100m CPU and 200Mi memory to a container in our cluster. We configure addon-resizer to add 1m CPU and 2Mi memory per node. When the cluster scales to 75 nodes, addon-resizer will scale the target container using the formula below:</p>
<pre><code>100m+(1m*75) = 175m
</code></pre>
<p>It will also increase memory:</p>
<pre><code>200Mi + (2Mi*75)= 350Mi
</code></pre>
<h2 id="scale-metrics-server">Scale metrics-server</h2>
<p>The first question you may have is when should you scale metrics-server. The default resource configuration in metrics-server Deployment is recommended for clusters of up to 100 nodes. Beyond that you may notice the metrics-server restarting frequently (as it gets killed by Kubernetes Out of Memory killer).</p>
<p>When metrics-server needs more resources than allocated <code>kubectl top nodes</code> and <code>kubectl top pods</code> will fail. You may get the following error message:</p>
<p><code>unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)</code></p>
<p>Also, the Horizontal Pod Autoscaler may stop working. If you run <code>kubectl get apiservices v1beta1.metrics.k8s.io</code>, you may get the following message:</p>
<pre><code>NAME                     SERVICE                      AVAILABLE                      AGE
v1beta1.metrics.k8s.io   kube-system/metrics-server   False (FailedDiscoveryCheck)   12m
</code></pre>
<h2 id="deploy-addon-resizer">Deploy addon-resizer</h2>
<p>The addon-resizer container can run in its own Pod or as a sidecar. We&#x2019;re going to deploy the container as a sidecar in the metrics-server Deployment.</p>
<p>The metrics server defaults to 100m CPU and 200Mi memory. Get the current limits:</p>
<pre><code>kubectl -n kube-system get \
  deployments metrics-server \
  -o jsonpath=&apos;{.spec.template.spec.containers[].resources}&apos;
</code></pre>
<p>Here&#x2019;s the output from my cluster:</p>
<pre><code>{&quot;requests&quot;:{&quot;cpu&quot;:&quot;100m&quot;,&quot;memory&quot;:&quot;200Mi&quot;}}
</code></pre>
<p>My cluster currently has 5 nodes. I&#x2019;ll configure the addon-resizer to scale the metrics-server Deployment vertically by adding 1m CPU per node in addition to the base CPU which is set to 20m. The base memory is 15Mi, and addon-resizer will increase metrics-server memory by 2Mi per node. I took these values from addon-resizer recommendations.</p>
<p>Deploy the manifest to create a ClusterRole, Role, and ClusterRoleBinding that gives the metrics-server service account the permissions to patch the metrics-server Deployment:</p>
<pre><code>cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: eks:metrics-server-nanny
  labels:
    k8s-app: metrics-server
rules:
- nonResourceURLs:
  - /metrics
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: eks:metrics-server-nanny
  labels:
    k8s-app: metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: eks:metrics-server-nanny
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: metrics-server-nanny
  namespace: kube-system
  labels:
    k8s-app: metrics-server
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - apps
  resources:
  - deployments
  resourceNames:
  - metrics-server
  verbs:
  - get
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-nanny
  namespace: kube-system
  labels:
    k8s-app: metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: metrics-server-nanny
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
EOF
</code></pre>
<p>Create a patch file to add the nanny container to the metrics-server Deployment.</p>
<pre><code>cat &gt; metrics-server-addon-patch.yaml &lt;&lt; EOF
spec:
  template:
    spec:
      containers:
      - name: metrics-server-nanny
          image: registry.k8s.io/autoscaling/addon-resizer:1.8.14
          resources:
            limits:
              cpu: 40m
              memory: 25Mi
            requests:
              cpu: 40m
              memory: 25Mi
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          command:
            - /pod_nanny
            - --cpu=20m
            - --extra-cpu=1m
            - --memory=15Mi
            - --extra-memory=2Mi
            - --threshold=5
            - --deployment=metrics-server
            - --container=metrics-server
            - --poll-period=30000
            - --estimator=exponential
            - --minClusterSize=10
            - --use-metrics=true
EOF

kubectl -n kube-system patch deployments metrics-server --patch-file metrics-server-addon-patch.yaml
</code></pre>
<p>If you install the metrics-server as documented in Amazon EKS documentation, it requests 100m CPU and 200Mi memory. After deploying the patch above, the metrics server requests set to 40m CPU and 15Mi memory. As you add more nodes, the nanny will automatically adjust the requests and limits for the metrics-server container.</p>
<p>I scaled my cluster to 15 nodes and the addon-resizer configured metrics-server requests to 35m CPU and 45Mi memory.</p>
<pre><code>20baseCPU+(15nodes*1extraCPU) = 35m

</code></pre>
<p>Memory calculation</p>
<pre><code>15baseMemory+(15nodes*2extraMemory) = 45Mi

</code></pre>
<p>Addon-resizer calculates the resources reservation for the metrics-server container and  restarts the container automatically.</p>
<h2 id="what-about-scaling-metrics-server-horizontally">What about scaling metrics server horizontally?</h2>
<p>While you can run metrics server in high-availability mode, its main purpose is ensuring that if one of the metrics server Pods terminate, the other one can still serve requests.</p>
<p>Running two instances of metrics server doesn&#x2019;t provide any further benefits. Both instances will scrape all nodes to collect metrics, but only one instance will be actively serving metrics API.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The Amazon EKS documentation currently documents steps to deploy metrics-server in static configuration. You can use addon-resizer to autoscale the metrics-server based on the number of nodes in your cluster.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Using eStargz to reduce container startup time on Amazon EKS]]></title><description><![CDATA[Large containers are slow to start. eStargz snapshotter gets container image layers on the fly. ]]></description><link>https://realvz.github.io/blog/using-estargz-to-reduce-container-startup-time-on-amazon-eks/</link><guid isPermaLink="false">63586d3998354e00013cefdf</guid><category><![CDATA[Amazon EKS]]></category><category><![CDATA[Kubernetes]]></category><category><![CDATA[Containers]]></category><category><![CDATA[Containerd]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Tue, 25 Oct 2022 23:11:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1578923931302-7fd9b3495be7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDExfHxob3VyZ2xhc3N8ZW58MHx8fHwxNjY2NzM5NTc3&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1578923931302-7fd9b3495be7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDExfHxob3VyZ2xhc3N8ZW58MHx8fHwxNjY2NzM5NTc3&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Using eStargz to reduce container startup time on Amazon EKS"><p>A container image bundles executable code, library, and configuration. Images contain everything an application needs to run. It is a best practice to exclude any file that&#x2019;s unnecessary for the application packaged in the image. A smaller image means that when you create a container, the container runtime (dockerd or containerd) will have to download fewer bits from the container registry, which will result in faster startup time.</p>
<p>There are cases when the container image becomes significantly large (&gt;500 MB). A common scenario is machine learning workloads. ML workload containers usually package model data necessary for the application. The image size for these containers can easily span in to multiple GBs. As a result, these applications have a slower startup time when the runtime has to pull the image. In fact, <a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter?ref=blog.realvarez.com">research</a> shows that pulling packages accounts for 76% of container start time, but only 6.4% of that data is read.</p>
<h2 id="lazy-pulling">Lazy-pulling</h2>
<p>There are two open source projects designed to improve container startup time. <a href="https://github.com/containerd/stargz-snapshotter?ref=blog.realvarez.com">Stargz</a> and <a href="https://github.com/awslabs/soci-snapshotter?ref=blog.realvarez.com">SOCI</a> are containerd plugins that reduce the cold start time by providing a way to run containers without downloading the entire image. They introduce the concept of <em>lazy pulling</em>, a technique that allows the runtime to download the bits from the container registry as needed.</p>
<p>Using lazy pulling significantly reduces the application startup time. eStargz is a lazily-pullable image format that is compatible with <a href="https://github.com/opencontainers/runtime-spec?ref=blog.realvarez.com">OCI runtimes</a> and standard container registries like DockerHub, GitHub Container Registry.</p>
<h2 id="estargz">eStargz</h2>
<p>The eStargz image format is based on stargz image format by Container Registry Filesystem (CRFS) open source project. <strong>CRFS</strong> is a read-only FUSE filesystem that lets you mount a container image, served directly from a container registry, without pulling it all locally first. The project introduces <strong>S</strong>eekable tar.gz format, which makes tar.gz files seekable using an index.</p>
<h2 id="stargz-snapshotter-plugin">Stargz Snapshotter plugin</h2>
<p>Stargz snapshotter is implemented as a <a href="https://github.com/containerd/containerd/blob/04985039cede6aafbb7dfb3206c9c4d04e2f924d/PLUGINS.md?ref=blog.realvarez.com#proxy-plugins">proxy plugin</a> daemon (<code>containerd-stargz-grpc</code>) for containerd. When containerd starts a container, it queries the rootfs snapshots to stargz snapshotter daemon through a unix socket. This snapshotter remotely mounts queried eStargz layers from registries on the node and provides these mount points as remote snapshots to containerd. The plugin uses FUSE to mount eStargz layers directly from the container registry.</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/28146E5F-3F27-4D41-94B7-CF6EF7615505/C70CF5ED-93B1-43E2-B0C2-41D09499BBC8_2/kxUcYlMPhkl9lEW9PWLWDQpsZcldhni1S81OJseSGZgz/Image.tiff" alt="Using eStargz to reduce container startup time on Amazon EKS" loading="lazy"></p>
<h2 id="running-estargz-images-on-amazon-eks">Running eStargz images on Amazon EKS</h2>
<p>eStargz images are a little different from the images you&#x2019;d build using <code>docker build</code>. In order to create an image that supports lazy pulling, you&apos;ll need an eStargz-aware image builder or a converter.</p>
<p>I am going to build my eStargz image using nerdctl, which is an eStargz-aware image builder. Since image size is not an issue, I will use Debian Jessie as the base image to demo. To make the image size artificially large, I will include twenty 50 MB files.</p>
<p>Lets generate large files containing random text:</p>
<pre><code>mkdir files
for i in {0..20}; do base64 /dev/urandom | head -c 50000000 &gt; files/file${i}.txt; done
</code></pre>
<p>Create a DockerFile:</p>
<pre><code>cat &gt; Dockerfile &lt;&lt;EOF
FROM debian:jessie
RUN apt-get update &amp;&amp; apt-get install -y \
    vim
COPY files .

EOF
</code></pre>
<p>I am going to store my image in Amazon ECR. I&#x2019;ll create an ECR repository:</p>
<pre><code>ECR_URI=$(aws ecr create-repository \
  --repository-name estargz-demo \
  --query &apos;repository.repositoryUri&apos; \
  --output text)
</code></pre>
<p>Use nerdctl to create container image:</p>
<pre><code>sudo nerdctl build -t ${ECR_URI}:1 .
</code></pre>
<p>The image is not in eStargz formatted right now, I&#x2019;ll have to convert it:</p>
<pre><code>nerdctl image convert --estargz --oci \
  ${ECR_URI}:1 ${ECR_URI}:1-esgz
</code></pre>
<p>The resulting images:</p>
<pre><code>nerdctl images
REPOSITORY                                                   TAG       IMAGE ID        CREATED           PLATFORM       SIZE       BLOB SIZE
account.dkr.ecr.us-west-2.amazonaws.com/estargz-demo    1         798b85a131ed    31 minutes ago    linux/amd64    1.2 GiB    828.8 MiB
account.dkr.ecr.us-west-2.amazonaws.com/estargz-demo    1-esgz    81b0ffd2a4a3    36 seconds ago    linux/amd64    0.0 B      832.3 MiB
</code></pre>
<p>Login to ECR and push the image to ECR:</p>
<pre><code>aws ecr get-login-password | sudo nerdctl login \
   --username AWS \
   --password-stdin \
   $ECR_URI
nerdctl push ${ECR_URI}:1-esgz
</code></pre>
<p>The image is now available in ECR and I can start running it in my EKS cluster.</p>
<h2 id="create-a-managed-node-group-that-uses-containerd">Create a managed node group that uses containerd</h2>
<p>EKS nodes currently don&#x2019;t enable containerd by default. I&#x2019;ll create a node group that uses containerd.</p>
<p>Create environment variables for AWS Region and EKS cluster name:</p>
<pre><code>AWS_REGION=&lt;Your AWS Region&gt;
CLUSTER_NAME=&lt;Your EKS cluster&apos;s name&gt;
</code></pre>
<p>First, I need to retrieve the id of the EKS optimized AMI in my region:</p>
<pre><code>aws ssm get-parameter --name /aws/service/eks/optimized-ami/1.23/amazon-linux-2/recommended/image_id --region us-west-2 --query &quot;Parameter.Value&quot; --output text
</code></pre>
<pre><code>cat &gt; containerd-mng.yaml &lt;&lt;EOF
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: $CLUSTER_NAME
  region: $AWS_REGION

managedNodeGroups:
  - name: containerd-mng
    minSize: 1
    maxSize: 1
    desiredCapacity: 1
    instanceType: m5.xlarge
    ami: $AMI_ID
    overrideBootstrapCommand: |
      #!/bin/bash
      /etc/eks/bootstrap.sh Socrates --container-runtime containerd
    
EOF
</code></pre>
<h2 id="preparing-eks-nodes-to-use-estargz">Preparing EKS nodes to use eStargz</h2>
<p>Next, I need to install eStargz snapshotter plugin on my worker node. I use AWS Systems Manager on my nodes, so I will connect to the containerd node.</p>
<pre><code>aws ssm start-session --target &lt;Instance ID of the worker node&gt;
</code></pre>
<p>Connect to the node (using ssh or Systems Manager) and back up the current containerd config file (/etc/containerd/config.toml) and replace it with:</p>
<pre><code>sudo mv /etc/containerd/config.toml /etc/containerd/config.toml.bak
cat &gt; config.toml &lt;&lt;EOF
version = 2
root = &quot;/var/lib/containerd&quot;
state = &quot;/run/containerd&quot;

[grpc]
address = &quot;/run/containerd/containerd.sock&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]
default_runtime_name = &quot;runc&quot;
snapshotter = &quot;stargz&quot;
disable_snapshot_annotations = false

[plugins.&quot;io.containerd.grpc.v1.cri&quot;]
sandbox_image = &quot;602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/pause:3.5&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
runtime_type = &quot;io.containerd.runc.v2&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
SystemdCgroup = true

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni]
bin_dir = &quot;/opt/cni/bin&quot;
conf_dir = &quot;/etc/cni/net.d&quot;

[proxy_plugins]
  [proxy_plugins.stargz]
    type = &quot;snapshot&quot;
    address = &quot;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&quot;
EOF
sudo mv config.toml /etc/containerd/config.toml
</code></pre>
<p>Install FUSE:</p>
<pre><code>sudo yum install fuse -y
sudo modprobe fuse
sudo bash -c &apos;echo &quot;fuse&quot; &gt; /etc/modules-load.d/fuse.conf&apos;
</code></pre>
<p>Install the snapshotter from its <a href="https://github.com/containerd/stargz-snapshotter/releases?ref=blog.realvarez.com">GitHub</a> repository:</p>
<pre><code>wget https://github.com/containerd/stargz-snapshotter/releases/download/v0.12.1/stargz-snapshotter-v0.12.1-linux-amd64.tar.gz
sudo tar xvzf stargz-snapshotter-v0.12.1-linux-amd64.tar.gz -C /usr/local/bin
sudo wget -O /etc/systemd/system/stargz-snapshotter.service https://raw.githubusercontent.com/containerd/stargz-snapshotter/main/script/config/etc/systemd/system/stargz-snapshotter.service
sudo systemctl enable --now stargz-snapshotter
</code></pre>
<p>Finally, restart the containerd and kubelet:</p>
<pre><code>sudo systemctl restart containerd
sudo systemctl restart kubelet
</code></pre>
<h2 id="test-lazy-pulling">Test lazy-pulling</h2>
<p>I will now run the pod using my eStargz formatted image. Create a manifest for a new pod and replace the node name with the newly created node:</p>
<pre><code>cat &gt; estargz-pod.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: stargz-demo
spec:
  containers:
  - name: stargz-demo
    image: ${ECR_URI}:1-esgz
    command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
    args: [ &quot;while true; do sleep 30; done;&quot; ]
  nodeName: ip-192-168-32-236.us-west-2.compute.internal
EOF
</code></pre>
<p>The pod started in 2 seconds:</p>
<pre><code>k get pods
NAME                            READY   STATUS        RESTARTS      AGE
stargz-demo                     1/1     Running       0             2s
</code></pre>
<p>To compare, I ran the same pod on another node that didn&#x2019;t use containerd. It took 45 seconds to start the same image. That&#x2019;s a lot of improvement in pod startup time!!</p>
<p>Image pull time <em>without</em> eStargz:</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/28146E5F-3F27-4D41-94B7-CF6EF7615505/46004B64-228E-4F69-B607-D0FBBF391149_2/AbByT341edypWUhHSsEY1F3qSAmSIf9KuHJwYVgzJQ0z/Image.jpeg" alt="Using eStargz to reduce container startup time on Amazon EKS" loading="lazy"></p>
<p>Image pull with eStargz:</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/28146E5F-3F27-4D41-94B7-CF6EF7615505/495F26BE-BA38-48DF-B3F6-9915FDF52E5C_2/N7tCcNUyi2GGlpNmWfwIzNnBzbdmKsxhWnhSRUVkw7Mz/Image.jpeg" alt="Using eStargz to reduce container startup time on Amazon EKS" loading="lazy"></p>
<h2 id="prefetching-files">Prefetching files</h2>
<p>What if you wanted the runtime to always download a file and disable lazy-pulling?</p>
<p>eStargz supports prefetching of files. This mitigates runtime performance drawbacks caused by the on-demand fetching of each file.</p>
<p>The example below always pulls <code>ls</code> and <code>bash</code> files before starting the container:</p>
<pre><code>$ cat &lt;&lt;EOF &gt; /tmp/record.json
{ &quot;path&quot; : &quot;/usr/bin/bash&quot; }
{ &quot;path&quot; : &quot;/usr/bin/ls&quot; }
EOF
$ nerdctl image convert --estargz --oci \
    --estargz-record-in=/tmp/record.json \
    ubuntu:21.04 ubuntu:21.04-ls
</code></pre>
<h2 id="seekable-oci-soci">Seekable OCI (SOCI)</h2>
<p>Seekable OCI (SOCI) is a technology open sourced by AWS that enables containers to launch faster by lazily loading the container image. SOCI works by creating an index (SOCI Index) of the files within an existing container image. This index is a key enabler to launching containers faster, providing the capability of extracting an individual file from a container image before downloading the entire archive.</p>
<p>SOCI borrows some of the design principles from stargz-snapshotter, but takes a different approach.</p>
<p>A SOCI index is generated separately from the container image, and is stored in the registry as an <a href="https://github.com/opencontainers/artifacts?ref=blog.realvarez.com">OCI Artifact</a> and linked back to the container image by <a href="https://github.com/opencontainers/tob/blob/main/proposals/wg-reference-types.md?ref=blog.realvarez.com">OCI Reference Types</a>. This means that the container images do not need to be converted, image digests do not change, and image signatures remain valid.</p>
<p>Most OCI registries like DockerHub and ECR do not currently support the &quot;referrers&quot; feature. So you cannot use SOCI unless you run a local <a href="https://oras.land/?ref=blog.realvarez.com">ORAS</a> registry.</p>
<h2 id="conclusion">Conclusion</h2>
<p>If you are looking to reduce container start time for your workloads, eStargz snapshotter is definitely worth a look. You&#x2019;ll have to change your existing container build pipelines to add a step to convert images though. When SOCI support is available in OCI registries, you&#x2019;ll be able to lazily-pull images without converting them first.</p>
<p>You&#x2019;ll also have to install eStargz snapshotter on your nodes and configure containerd to use the plugin. Creating a custom AMI or using AWS Systems manager will be the best way to do that. You could also use a DaemonSet to configure the system, but keep in mind that you&#x2019;ll need to account for restarting the containerd and kubelet.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Handling Feedback]]></title><description><![CDATA[<p>Accepting feedback with grace plays a key role in building long lasting productive relationships. Humans are social creatures and feedback fuels our psyche. We nod when we see our colleagues in the hallway. Sometimes multiple times a day. We wave to our neighbors we rarely know, and we seek recognition</p>]]></description><link>https://realvz.github.io/blog/handling-feedback/</link><guid isPermaLink="false">6343237098354e00013cefb6</guid><category><![CDATA[Feedback]]></category><category><![CDATA[Productivity]]></category><category><![CDATA[Ledaership]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Sun, 09 Oct 2022 19:40:20 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxncm93dGh8ZW58MHx8fHwxNjY1MzQ0Mzg5&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxncm93dGh8ZW58MHx8fHwxNjY1MzQ0Mzg5&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Handling Feedback"><p>Accepting feedback with grace plays a key role in building long lasting productive relationships. Humans are social creatures and feedback fuels our psyche. We nod when we see our colleagues in the hallway. Sometimes multiple times a day. We wave to our neighbors we rarely know, and we seek recognition from those we look up to.</p><p>While positive feedback is invigorating, negative feedback about something we care deeply about can hinder our creativity. Especially when negative feedback carries hints of criticism and personal attacks.</p><p>I have been in so many situations where leaders crush inventiveness with their <em>brutally honest</em> facts. These leaders think they are providing feedback, but they end up shaming attempts at innovation. Although inadvertent, their methods focus on individual rather than the product they are supposed to be improving.</p><p>A survey by <a href="https://www.talentinnovation.org/?ref=blog.realvarez.com">the Center of Talent Innovation</a> found that when undermining management causes burnouts. This leadership style fosters an environment that becomes hostile to failure, and therefore, innovation.</p><p>Collaborative creativity thrives in <em>psychologically safe</em> environments. When people believe others will not penalize them or think less of them for mistakes, they are more likely to take chances and think outside of the box. Isn&#x2019;t that what management asks us all to do?</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/85595440-9377-4C01-B155-23C96FE4B91B/DB390C2D-FEE3-42F8-928D-96FEA52E0FA4_2/FGxQcbVgE38Dm7OknbK4Krt5tw9Ym1Z3b9J2aGRpcQoz/Image.png" class="kg-image" alt="Handling Feedback" loading="lazy" width="1170" height="1276"></figure><p>Harvard Business School professor Amy Edmondson in <a href="https://web.mit.edu/curhan/www/docs/Articles/15341_Readings/Group_Performance/Edmondson%20Psychological%20safety.pdf?ref=blog.realvarez.com">Psychological Safety and Learning Behavior in Work Teams</a> writes:</p><blockquote>Team psychological safety should facilitate learning behavior in work teams because it eases excessive concern about others&#x2019; reactions to actions that have the potential for embarrassment or threat, which learning behaviors often have. For example, team members may be unwilling to bring up errors that could help the team make subsequent changes because they are concerned about being seen as incompetent, which allows them to ignore or discount the negative consequences of their silence for team performance. In contrast, if they respect and feel respected by other team members and feel confident that team members will not hold the error against them, the benefits of speaking up are likely to be given more weight.</blockquote><p>Negative criticism may be discouraging, whether you&apos;re getting it for a creative project or at a performance review. Many of us can recall at least one occasion when we were told that we were not good enough. Brene Brown refers to these moments as <em>creativity scars.</em> Brown learned that &#xA0;85% people had a school incident in their childhood that was so shaming, it changed their perception about themselves.</p><p>It remains an unfortunate fact that creativity, despite its inherent challenges, has to survive in environments designed to crush it. Pianists, singer, actors, or software developers, we all fight against the failure and self-doubt, not only in themselves but also in others. Open any business book and you&#x2019;ll hear story after story of interpersonal conflicts and team dynamics stifling innovation.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/85595440-9377-4C01-B155-23C96FE4B91B/29954455-96B5-4109-9D48-C7F80A2FB741_2/7M1InpeGb4ctAfTV1fWfObn8IwAuUSnOsfyWPR1gha0z/Image.tiff" class="kg-image" alt="Handling Feedback" loading="lazy"></figure><p>Negative feedback is everyone&#x2019;s reality. I am staunch believer that <em>mastery requires feedback</em>. Maturity is in understanding that everyone fails. Failure is in remaining knocked down. Getting back into the game, and learning from your mistakes, is the way to fight failure. Even superstars are not immune to failure and criticism.</p><p>One way to reduce negative feedback is by selecting the &#x201C;right people&#x201D;. This is an unreasonable expectation in professional environments. Or in <em>any</em> environment. I am not suggesting that you should care about everyone&#x2019;s opinion. However, listening to a limited set of people can lead you in the wrong direction or groupthink. I am a proponent of diversity, and that includes seeking feedback from folks across the spectrum.</p><h3 id="maintaining-growth-mindset"><strong>Maintaining growth mindset</strong></h3><p>I have learned that most feedback recipients need to carry a sieve to sift through the message. Surely there will be times when you&#x2019;ll find nothing worthy to keep. But assuming you are asking the people that truly care about improving your contributions, there will be nuggets of good ideas to help you improve.</p><p>When receiving negative (or poorly presented) feedback, you have two options. First is to simply ignore it, which might be great if you are an unconventional genius like Dostoevsky or Lady Gaga. The other option is to validate the feedback and seek things that can polish your work. I deal with the second situation. Most of the time.</p><p>The worst you can do in these cases is to focus on the hurtful comments. Instead, listen. Be prepared to accept the feedback. Becoming defensive will hinder your ability to understand diverse viewpoints.</p><p>Remain gracious as being thankful can disarm people intending to hurt you. Thank people for their time and effort. Don&#x2019;t sound flippant or use sarcasm to shield yourself. Use this opportunity to learn. Don&#x2019;t damage your relationships.</p><blockquote>Brown writes <em>&#x201C;don&#x2019;t grab hurtful comments and pull them close to you by rereading them and ruminating on them. Don&#x2019;t play with them by rehearsing your badass comeback. And whatever you do, don&#x2019;t pull hatefulness close to your heart.&#x201D;</em></blockquote><p>When I recently faced hurtful feedback, I asked myself why does it sting. I carried elaborate chats with my opponent in my head. At first, I tried very hard to prove them wrong. Then I tried to disqualify their arguments. Then I discredited them. &#x201C;What do they know?&#x201D;</p><p>They had hurt me with their comments. I allowed myself to wallow. I tried to put my feelings into words. In retrospective, the best thing I did was to avoid belittling myself, which I knew would&#x2019;ve been a giant waste of time. I realized that the feedback was hard to process because it attacked my vulnerabilities. It had questioned my credibility.</p><p>Many of us carry self-doubt deep within ourselves. There&#x2019;s a strong link that connects our insecurities and the things that are important to us. We are worried that we&#x2019;re not as competent or informed as we (or others) think we are. We live in a perpetual state of <a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=imposter+syndrome&amp;ie=UTF-8&amp;oe=UTF-8&amp;ref=blog.realvarez.com"><em>imposter syndrome</em></a>.</p><p>For reassurance, I remembered my past accomplishments. Practiced a bit of self-compassion. I couldn&#x2019;t let an opinion define who I was. I had to isolate the feedback and the feelings it had brought to surface, and continue with my productive journey onward.</p><p>The feedback provider had evoked my insecurities. The negative feelings triggered my defense mechanism setting my brain into flight or fight mode. In defensive mode, I was spending more energy fighting the feedback, berating myself, and feeling inadequate than using the feedback to improve my output. My fears made me think that the person saw my <em>authentic</em> version, the one that&#x2019;s a total failure.</p><p>It took me a couple of days to <em>calm down</em>. It couldn&#x2019;t have been possible if I hadn&#x2019;t processed my feelings and analyzed my thoughts. Ultimately, I did succeed in putting aside the negative stuff and focusing on the constructive feedback.</p><p>In this situation, I realized that the feedback did make sense, and if roles were switched, I would&#x2019;ve made the same recommendations. Albeit in a more professional and positive way.</p><h3 id="set-clear-boundaries"><strong>Set clear boundaries</strong></h3><p>The incident definitely taught me that good feedback can sound outrageous at first (vice versa is also true). I am certainly not going to exclude people with tough feedback nor the ones that don&#x2019;t know how to provide it. I am now more aware that my insecurities can make me stop listening and hinder my creativity.</p><p>In fact, I intend to go a step further and welcome negative feedback. Not because I am a glutton for punishment, but because I know people struggle to provide positive feedback.</p><p>When soliciting feedback, consider being specific and request feedback frequently. Doing so will give you inputs in more manageable portions.</p><p>The right way to provide feedback is to put the problem in focus and not personalities. I positive feedback session for me is where I don&#x2019;t come across as lecturer. I try to identify strengths in teams and individuals, and instances where that strength has potential to grow. The requestor and provider have to remain respectful at all times. I know people will not accept my feedback if I shame or blame them.</p><p>When providing feedback, I need to choose my words wisely. The recipient is asking for help, not judgement. I can avoid sounding negative by first seeking permission to provide feedback and clearly defining the scope of feedback. By setting boundaries, I hope to ensure not stepping outside the scope of feedback I am being asked to provide. I cannot erode trust in relationships by sounding egoistic, self-righteous, and becoming an impediment to innovation.</p>]]></content:encoded></item><item><title><![CDATA[How is eBPF efficient for observability]]></title><description><![CDATA[A look at how eBPF efficiently facilitates  information exchanges between the user and Linux kernel space. ]]></description><link>https://realvz.github.io/blog/how-is-ebpf-efficient-for-observability/</link><guid isPermaLink="false">62c9adb74f5158000176d75d</guid><category><![CDATA[Linux]]></category><category><![CDATA[Observability]]></category><category><![CDATA[eBPF]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Sat, 09 Jul 2022 16:37:45 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1514732800601-524d62e9b8a7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDk3fHxyZWQlMjBhbmQlMjBibHVlJTIwbGlnaHR8ZW58MHx8fHwxNjU3Mzg0NTY1&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<blockquote>Everybody knows eBPF is fast, but how?</blockquote><img src="https://images.unsplash.com/photo-1514732800601-524d62e9b8a7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDk3fHxyZWQlMjBhbmQlMjBibHVlJTIwbGlnaHR8ZW58MHx8fHwxNjU3Mzg0NTY1&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="How is eBPF efficient for observability"><p>The Berkeley Packet Filter (BPF or eBPF) is a virtual machine based on registers, initially designed for filtering network packets, best known for its use in <code>tcpdump</code>. In this post, we learn what is exactly that makes BPF so appealing for a number of use cases.</p><h2 id="introduction">Introduction</h2><p>BPF programs are small programs that run in the Linux kernel when events occur. You can think of BPF programs as event driven functions (like AWS Lambda). &#xA0;BPF has access to a subset of kernel functions and memory. It is designed to be safe, that is poor BPF code won&#x2019;t crash your kernel.</p><p>In the containers world, BPF is increasingly becoming relevant. Its most popular use case is Cilium, which uses eBPF to provide a <a href="https://cilium.io/blog/2021/12/01/cilium-service-mesh-beta?ref=blog.realvarez.com">&#x201C;sidecarless&#x201D; service mesh</a> and <a href="https://github.com/pixie-io/pixie?ref=blog.realvarez.com">Pixie</a>, which uses eBPF to collect telemetry data.</p><p>Other popular BPF uses are:</p><ul><li><strong>Debugging and Tracing -</strong> trace any <code>syscall</code> or kernel function or any user space program.</li><li><a href="https://github.com/iovisor/bpftrace?ref=blog.realvarez.com">bpftrace</a> allows users to trace from the Linux command line.</li><li><strong>Networking -</strong> Inspect, filter, manipulate traffic.</li><li>User space program can attach a filter to any socket and inspect the traffic flowing through it. They can also allow, disallow, redirect packets.</li><li><strong>Security monitoring and sandboxing</strong></li><li>BPF programs can detect and report the <code>syscalls</code> occurring on a system.</li><li>BPF programs can prevent applications from performing certain <code>syscalls</code> on a system (e.g., prevent deleting a file).</li></ul><p>Linux has allowed us to perform above functions for decades, but BPF helps us perform these tasks more efficiently. BPF programs use fewer CPU and memory resources than traditional solutions.</p><h2 id="how-is-bpf-faster">How is BPF faster?</h2><p>BPF programs are faster because BPF code runs in kernel space.</p><p>Consider the steps a program has to take to calculate the number of bytes sent out on a Linux system. First, the kernel generates raw data as network activity occurs. This raw data packs a ton of information, most of which is irrelevant for calculating <em>bytes out</em> metrics. So, whatever is generating aggregated metrics will have to &#xA0;filter the relevant data points repeatedly and run mathematical calculations on them. This process is repeated hundreds of times (or more) every minute.</p><p>Since traditional monitoring programs run in user space, all that raw data that the kernel generates has to be copied from kernel space into user space. This data copy and filtering operation can be very taxing on the CPU. This is why <code>ptrace</code> is slow (whereas <code><a href="https://github.com/iovisor/bpftrace?ref=blog.realvarez.com">bpftrace</a></code> is not).</p><p>eBPF avoids this copying of data from kernel space to user space. You can run a program in kernel space to aggregate the data you need and send just the output to user space.</p><p>Before BPF, large amounts of raw data from the kernel had to be copied over to user space for analysis. BPF allows creating histograms and filtering data within the kernel, which is much faster than exchanging massive amounts of data between user and kernel space.</p><h2 id="bpf-maps">BPF Maps</h2><p>BPF uses <strong>BPF maps</strong> to allow bidirectional data exchange between user and kernel space. In Linux, maps are a generic storage type for sharing data between user and kernel space. They are key value stores that reside in the kernel.</p><h2 id="for-metrics-generation-bpf-programs-run-calculations-in-kernel-space-and-write-results-to-bpf-maps-that-a-userspace-application-can-read-and-also-write-to">For metrics generation, BPF programs run calculations in kernel space and write results to BPF maps that a userspace &#xA0;application can read (and also write to).</h2><p>Now you know why eBPF is efficient. It&#x2019;s because BPF provides a way to run programs in kernel space and avoid copying irrelevant data between kernel and userspace.</p><h2 id="references">References</h2><p><a href="https://thenewstack.io/how-ebpf-streamlines-the-service-mesh?ref=blog.realvarez.com">https://thenewstack.io/how-ebpf-streamlines-the-service-mesh</a></p><p><a href="https://buoyant.io/2022/06/07/ebpf-sidecars-and-the-future-of-the-service-mesh/?ref=blog.realvarez.com">https://buoyant.io/2022/06/07/ebpf-sidecars-and-the-future-of-the-service-mesh/</a></p><p><a href="https://av.tib.eu/media/44349?ref=blog.realvarez.com">https://av.tib.eu/media/44349</a><a href="https://www.amazon.com/Linux-Observability-BPF-Programming-Performance/dp/1492050202?ref=blog.realvarez.com">https://www.amazon.com/Linux-Observability-BPF-Programming-Performance/dp/1492050202</a></p><p><a href="https://www.amazon.com/Linux-Observability-BPF-Programming-Performance/dp/1492050202?ref=blog.realvarez.com">https://www.amazon.com/Linux-Observability-BPF-Programming-Performance/dp/1492050202</a></p>]]></content:encoded></item><item><title><![CDATA[Weak References: A weapon against Out Of Memory Errors]]></title><description><![CDATA[Using weak references to build caches. ]]></description><link>https://realvz.github.io/blog/weak-references-a-weapon-against-out-of-memory-errors/</link><guid isPermaLink="false">62a75a92f171aa00011844f8</guid><category><![CDATA[Software Design]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Sun, 29 May 2022 03:21:09 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1487058792275-0ad4aaf24ca7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHN5c3RlbXN8ZW58MHx8fHwxNjUzNzk0NTU3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1487058792275-0ad4aaf24ca7?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHN5c3RlbXN8ZW58MHx8fHwxNjUzNzk0NTU3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Weak References: A weapon against Out Of Memory Errors"><p>One of the key features that containers offer us is the ability to apply resource control through <a href="https://man7.org/linux/man-pages/man7/cgroups.7.html?ref=blog.realvarez.com">Control Groups</a>, generally referred to as cgroups. Containers, which build on top of Linux cgroups, allow us to run process that have limited access to host&apos;s total capacity.</p>
<p>This ability allows us to co-locate containers on the same host without being concerned that a container may impact other containerized apps, the so called <em>noisy neighbor problem</em>.</p>
<p>While cgroups allow us to control many resources, the most used controls are CPU and memory. For scalable, multi-tenant clusters like Kubernetes or Amazon ECS, resource limiting is a best practice.</p>
<p>ECS and Kubernetes allow us to limit resource consumption at container and ECS task or Kubernetes pod level. Below are the requests and limits a pod can have:</p>
<ul>
<li>
<p><code>spec.containers[].resources.limits.cpu</code></p>
</li>
<li>
<p><code>spec.containers[].resources.limits.memory</code></p>
</li>
<li>
<p><code>spec.containers[].resources.limits.hugepages-</code></p>
</li>
<li>
<p><code>spec.containers[].resources.requests.cpu</code></p>
</li>
<li>
<p><code>spec.containers[].resources.requests.memory</code></p>
</li>
<li>
<p><code>spec.containers[].resources.requests.hugepages-</code></p>
</li>
</ul>
<p>Similarly, Docker desktop allows configuring memory and CPU limits. Should a container try to use more memory than allocated, it gets <em>killed</em>.</p>
<pre><code>[336546.736392] Out of memory: Kill process 9218 (java) score 330 or sacrifice child
[336546.738454] Killed process 9218 (java) total-vm:7543324kB, test-mem:1060364kB, test-mem2:0kB, test-mem-rar:0kB
[336547.071919] oom_reaper: reaped process 9218 (java), now test-mem:0kB
</code></pre>
<h2 id="memorylimits">Memory Limits</h2>
<p>Many enterprises are in the process of moving legacy applications to containers clusters. Some of these applications ran on dedicated hardware. Their business requirements didn&#x2019;t include operating with resource constraints.</p>
<p>Having dedicated resources meant that traditional applications were free to consume as much system resources as available. Although Java allows us to control the heap size, it is rarely used to restrict memory usage for production applications.</p>
<p>When these applications migrate to containers and a limit is put upon their memory consumption, new out of memory errors may emerge. A common remedy is to allocate more memory until the application stops crashing.</p>
<p>In many cases, that might be the best solution. Pay a bit more of RAM, and move on. Code rot makes changes expensive with each passing day. But if you do have the resources to refactor code, optimizing memory consumption will improve the reliability of your applications.</p>
<h2 id="garbagecollection">&#x1F9F9; Garbage Collection</h2>
<p>One question we may want to ask ourselves is: What are some of the common ways of using memory inefficiently? Once you&#x2019;ve fixed all the memory leaks, what&#x2019;s next?</p>
<p>C programmers had to manage memory manually using malloc() and free(). Thankfully, higher-level languages automatically allocate memory when programs create objects and free memory when the object is not required.</p>
<p>When a process creates a new object, the system will allocate it memory and return the pointer to the program. Typically, a program&#x2019;s variables (or objects) will remain in memory as long as the code is running. In programming languages like Python, objects include a reference count field, which counts how many objects are referencing it. JavaScript object has a reference to its prototype (implicit reference) and to its properties values (explicit reference). As long as an object has a non-zero reference, it&#x2019;s immune to garbage collection.</p>
<p>Once the process terminates, the garbage controller is free to remove the associated objects from memory as they don&#x2019;t have any references. Objects created like these have a strong reference, which is the default behavior in all programming languages. They remain in memory until the process terminates.</p>
<h3 id="weakreferencing">&#x1F449;&#x1F3FC; Weak Referencing</h3>
<p>One way to reduce memory consumption is by decreasing the amount and size of the objects stored in memory.</p>
<p>Many languages also support creating objects with weak reference. Garbage collectors can remove any variable stored with weak reference even while the process that created it is still running. This provides an excellent way to store objects in memory that can be safely removed if the system comes under memory pressure.</p>
<p>Weak Reference objects allow applications to safely build caches without risking running out of memory.</p>
<h3 id="downsidesofweakreferencing">&#x1F3C1; Downsides of weak referencing</h3>
<p>Weak referencing forms the foundation for caching. And, similar to caching, it requires *getting and setting *a variable before usage. So, checking a variable if it exists before reading or writing to it is necessary for weak references.</p>
<p>Can you imaging what would happen if a process tries to read its variable that&#x2019;s stored as a weak reference, and the garbage controller has deleted it? Nothing good.</p>
<h3 id="differentlevelsofweakreferences">&#x1F39A; Different levels of weak references</h3>
<p>In addition to the weak references described above, languages like Java provide *Soft References *and *Phantom References. *Here&apos;s an <a href="https://dzone.com/articles/weak-soft-and-phantom-references-in-java-and-why-they-matter?ref=blog.realvarez.com">article</a> that goes over the differences. The TLDR is that the garbage collector removes weak references first, then soft references, and finally phantom references.</p>
<p>If your language provides different levels of weak references, consider choosing one that suits your application&apos;s requirements.</p>
<h3 id="externalcaching">&#x1FAF1;&#x1F3FD;&#x200D;&#x1FAF2;&#x1F3FC; External caching</h3>
<p>If your application runs multiple replicas or if multiple processes are caching similar data, consider using Redis or Memcached as an external, distributed, and shared cache. These caching solutions are excellent for web services as they allow multiple replicas to share a cache, which allow us to store user&#x2019;s sessions state without requiring sticky sessions.</p>
<h2 id="bonustip">&#x1F3C6; Bonus tip</h2>
<p>Another common cause for OOM errors in web applications is not keeping the payload size minimal. Web servers stores data for every session in memory until the session terminates. When a client requests a large payload, and the server doesn&#x2019;t limit it, the systems stores the payload in memory before it is sent to the client.</p>
<p>We can limit the payload size by restricting the maximum amount of data sent to a particular client. For example, you can paginate results from a database instead of sending the entire dataset to the client.</p>
<h2 id="references">References</h2>
<p><a href="https://docs.python.org/3/library/weakref.html?ref=blog.realvarez.com">weakref &#x2014; Weak references &#x2014; Python 3.10.4 documentation</a></p>
<p><a href="https://indradhanush.github.io/blog/life-of-a-container/?ref=blog.realvarez.com">Life of a Container</a></p>
<p><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Memory_Management?ref=blog.realvarez.com">Memory Management - JavaScript | MDN</a></p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Closing observability gaps with custom metrics]]></title><description><![CDATA[This post is an attempt to provide a list of metrics to collect in a typical microservice. ]]></description><link>https://realvz.github.io/blog/closing-observability-gaps-with-custom-metrics/</link><guid isPermaLink="false">62a75a92f171aa00011844f7</guid><category><![CDATA[Microservices]]></category><category><![CDATA[Software Design]]></category><category><![CDATA[Kubernetes]]></category><category><![CDATA[Containers]]></category><category><![CDATA[Observability]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Thu, 26 May 2022 23:53:18 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG1ldHJpY3N8ZW58MHx8fHwxNjUzNjA5MjQy&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><blockquote>
<img src="https://images.unsplash.com/photo-1551288049-bebda4e38f71?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG1ldHJpY3N8ZW58MHx8fHwxNjUzNjA5MjQy&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Closing observability gaps with custom metrics"><p><em>Which application metrics should you collect?</em></p>
</blockquote>
<p>I frequently engage with customers that are amid breaking their monolithic applications into smaller microservices. Many teams with also see this migration as an opportunity to make applications more observable. As a result, customers inquire which metrics they should monitor for a typical cloud native application.</p>
<p>Previously, when a customer asked me how to instrument a service, I pointed them to the well known <a href="https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/?ref=blog.realvarez.com">USE and RED methods</a>. But, I felt the response wasn&#x2019;t thorough. A list of specific metrics to monitor can be helpful for teams building cloud native applications. This post is an attempt to provide a list of metrics to collect in a typical application. Not all the metrics listed below apply to every application type. For example, batch-like workloads rarely serve traffic, and resultantly, don&apos;t need to keep a log of requests-served.</p>
<p>The goal of this document is to help developers come up with the golden signals for their applications.</p>
<p>Golden Signals, a term used first in the <a href="https://sre.google/sre-book/monitoring-distributed-systems/?ref=blog.realvarez.com">Google SRE handbook</a>. Golden Signals are four metrics that will give you a very good idea of the real health and performance of your application as seen by the actors interacting with that service, whether they are final users or another service in your microservice application.</p>
<h2 id="observability">Observability</h2>
<p>Cloud best practices recommend building systems that are observable. While the word observability (or* *&#x201C;<em>O11y</em>&#x201D; as it is popularly known) doesn&#x2019;t have an official definition, it is the measure of a system&#x2019;s ability to expose its internal state. The three pillars of observability are logs, metrics, and traces.</p>
<p>Modern systems are designed to produce logs, <em>emit</em> metrics, and provide traces to help developers and operators understand its internal state.</p>
<p><a href="https://giedrius.blog/2019/05/11/push-vs-pull-in-monitoring-systems/?ref=blog.realvarez.com">Push vs Pull </a></p>
<p>Emitting metrics by exposing them on an externally accessible HTTP endpoint is gaining wider adoption thanks to developers adopting Prometheus for monitoring. In this model, Prometheus pulls metrics by scraping the application&#x2019;s <code>/metrics</code> endpoint.</p>
<p>When you run Node Exporter, it publishes metrics at <code>http://localhost:9100/metrics</code></p>
<p>Observability tools aggregate and analyze data from different sources to help you detect issues and identify bottlenecks. The goal is to use these system signals to improve its reliability and prevent downtime.</p>
<p>AIOps products like <a href="https://aws.amazon.com/devops-guru/?ref=blog.realvarez.com">Amazon DevOps Guru</a> can also detect anomalies using your application&apos;s logs, metrics, and traces (and other sources) and give you early signals to prevent a potential disruption.</p>
<h2 id="metrics-to-collect">Metrics to collect</h2>
<p>For an application to function as designed, the application and its underlying system have to be <em>healthy</em>. Host metrics inform the operator of the host&#x2019;s and infrastructure resource usage, like CPU, memory, I/O, etc. If you use Prometheus, <a href="https://github.com/prometheus/node_exporter?ref=blog.realvarez.com">Node Exporter</a> collects this information automatically for you.</p>
<p>Host metrics rarely differ. Whether we run a process on an EC2 instance or a Raspberry Pi, we&#x2019;re interested in the same metrics.</p>
<p>Unlike host metrics, application metrics are unique to each microservice. Application metrics are supposed to provide the operator the information so they can do these things:</p>
<ol>
<li>
<p>Identify future areas of improvement by providing code-specific measurements. Application monitoring or APM tools provide measurements over a segment of time that developers can analyze.</p>
</li>
<li>
<p>When the system fails, provide information for troubleshooting and prevention.</p>
</li>
<li>
<p>In some cases, provide early signals to business. For example, if the application exposes, the *orders *it has processed in the last 60 minutes can be tracked using the monitoring system, rather than querying a relational database.</p>
</li>
</ol>
<p>There are several companies like application monitoring or APM companies like New Relic, DataDog that have products to aggregate application metrics using SDKs or agents. However, what they will not collect are the business specific metrics that only your application cares about.</p>
<p>In order to create a list of relevant metrics for an application, its architects will need to determine a signal for its every key function. The hallmark of a microservice is that it does* one thing well*, therefore it shouldn&#x2019;t have many key functions. Start by white-boarding the functions implemented in the code and creating a list of metrics that would help you gauge its performance (or its availability at the least).</p>
<p>Most measurements you&#x2019;ll do will fall under one of these categories:</p>
<h4 id="counter">Counter</h4>
<p>As the name suggests, this value is incremented when a function runs. Example: total requests served</p>
<h4 id="histogram">Histogram</h4>
<p>A histogram samples observations (usually things like request durations or response sizes) and counts them in configurable buckets.</p>
<p><strong>Gauge</strong></p>
<p>This type is metric tracks a value that increases or decreases over a period. Example: number of threads.</p>
<hr>
<p>With that background, let&#x2019;s go through the list of common custom metrics developers use.</p>
<h3 id="network-activity">Network activity</h3>
<p>These are the obvious metrics to track for any application that serves traffic. Network metrics tell you how much load is placed on the system. Over the time, these data points assist you when devising the scaling strategy for the system.</p>
<p>Things you should include are:</p>
<ul>
<li>
<p>Request count by API type or page</p>
</li>
<li>
<p>Requests total</p>
</li>
<li>
<p>Transactions</p>
</li>
<li>
<p>Concurrent, expired, and rejected sessions</p>
</li>
<li>
<p>A watermark that records maximum concurrent sessions</p>
</li>
<li>
<p>Average processing time</p>
</li>
<li>
<p>A count by error type</p>
</li>
</ul>
<h3 id="resource-usage">Resource usage</h3>
<p>It is a best practice to monitor a systems <em>saturation</em>, which is a measure of your systems resource consumption. Every resource has a <em>breaking point</em>, beyond which additional stress causes performance degradation. Scalable and reliable systems are designed to never breach the breaking point.</p>
<p>However, simply collecting overall resource saturation at an application level is insufficient. You also need to look deeper at thread or resource pool level.</p>
<p>Consider collecting these metrics:</p>
<ul>
<li>
<p>Number of processors, system CPU load, process CPU load, available memory, used memory, available swap, used swap, open file descriptor count.</p>
</li>
<li>
<p>Total resources consumed by connection pools, thread pools, and any other resource pools.</p>
</li>
<li>
<p>Total started thread count, current thread count, current busy threads, keep alive count, poller thread count, and connection count.</p>
</li>
<li>
<p>Objects created, destroyed, and checked out, high-water mark, number of times checked out,</p>
</li>
<li>
<p>Number of threads blocked waiting for a resource, number of times a thread has blocked waiting</p>
</li>
</ul>
<p>Common frameworks like Tomcat, Flask, etc. support exporting pre-defined metrics. For example, JMX already exposes a bunch of these metrics. See <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights-Prometheus-Sample-Workloads-javajmx.html?ref=blog.realvarez.com">AWS CloudWatch documentation</a>.</p>
<h3 id="users">Users</h3>
<p>Besides, serving the intended audience, bots or scripts flood internet facing web servers with requests. These automated requests can overload the system if unauthenticated requests are improperly handled (for example, not redirecting all unauthenticated requests to the authentication service and attempting to process an unauthenticated request).</p>
<p>Here are user related metrics to collect:</p>
<ul>
<li>
<p>Authenticated and unauthenticated requests</p>
</li>
<li>
<p>Demographics, authenticated and unauthenticated requests, usage patterns,</p>
</li>
<li>
<p>Unsuccessful login attempts</p>
</li>
</ul>
<p>Some of these metrics may also come from your Load Balancer or ingress.</p>
<h3 id="business-transaction-for-each-type">Business transaction (for each type)</h3>
<p>If your application follows the microservices approach, then the code fulfills one function, at least that&#x2019;s the idea. What are the key performance indicators for your app&#x2019;s function? Define them and track these metrics.</p>
<p>Should future releases cause performance regression, you&#x2019;ll be able to detect it. Tracking these business metrics will help you track trends easily and avoid a cascading failure.</p>
<p>Here are common things that services care about:</p>
<ul>
<li>
<p>Orders, messages, requests, transactions processed</p>
</li>
<li>
<p>Success and failure rates. For a retailer, this could be the conversion rate.</p>
</li>
<li>
<p>Service level agreements (like average transaction response time)</p>
</li>
</ul>
<p>If you still need help with identifying key metrics, ask yourself this question: In what ways can my application negatively affect the business even when it might appear to be healthy?</p>
<h3 id="database-connections">Database connections</h3>
<p>Along with monitoring your database instances using database monitoring tools, consider collecting database connection health metrics in your application. This is especially helpful if your application uses a shared database. If your application encounters database connection errors but the database remains operational for other application, you know the problem is on the application side, and not the database.</p>
<p>Consider recording these databases-related metrics:</p>
<ul>
<li>
<p>A count of <code>SQLException</code> thrown</p>
</li>
<li>
<p>Number of (concurrent or maximum)queries</p>
</li>
<li>
<p>Average query run time</p>
</li>
</ul>
<h3 id="data-consumption">Data consumption</h3>
<p>Wherever you&#x2019;re persisting data, you need to ensure that you&#x2019;re going to go over your quotas and run out of space. Besides, monitoring on disk and in-memory data volumes, don&#x2019;t forget to monitor the data your application stores in databases and caches.</p>
<h3 id="cache-health">Cache health</h3>
<p>Speaking of cache, it is a best practice to monitor these metrics:</p>
<ul>
<li>
<p>Items in cache</p>
</li>
<li>
<p>Get and set latency</p>
</li>
<li>
<p>Hits and miss rates</p>
</li>
<li>
<p>Items flushed</p>
</li>
</ul>
<p>Also, consider using an external cache such as Redis or Memcached.</p>
<h3 id="external-services">External services</h3>
<p>Keeping a track of how downstream services perform is also useful in understanding issues. Along with using timeouts, retries (preferably with <a href="https://en.wikipedia.org/wiki/Exponential_backoff?ref=blog.realvarez.com">exponential backoff</a>), and circuit breakers, consider monitoring these metrics for every external service your service&apos;s proper functioning depends on:</p>
<ul>
<li>
<p>Circuit breaker status</p>
</li>
<li>
<p>Count of timeouts, requests</p>
</li>
<li>
<p>Average response time or latency</p>
</li>
<li>
<p>Responses by type</p>
</li>
<li>
<p>Network errors, protocol errors</p>
</li>
<li>
<p>Requests in flight</p>
</li>
<li>
<p>A high watermark of concurrent requests.</p>
</li>
</ul>
<h3 id="granularity-in-metrics-collection">Granularity in metrics collection</h3>
<p>The frequency at which you publish and collect metrics depends on your business requirements. For a retailer, knowing traffic patterns by the hour and day is useful in scaling capacity. Similarly, a travel company&#x2019;s traffic pattern are influenced by holiday schedules.</p>
<p>Amazon EC2 provides instance metrics at 1-minute interval, which is a good start for critical metrics.</p>
<p>Remember that there&#x2019;s a cost attached to exposing, collecting, and analyzing metrics. Collecting unnecessary information in metrics can put a strain on the system and slow down troubleshooting.</p>
<p>Consider giving the operator the control over the metrics your code should generate. This way, you can turn on specific metrics whenever needed.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Finding out which metrics to collect is an answer that only the most familiar with the code can answer. This post provides a list of metrics for you to get started.</p>
<p>Are there any metrics that I have overlooked? Let me know at <a href="https://twitter.com/realz?ref=blog.realvarez.com">@realz</a>.</p>
<h2 id="references">References</h2>
<p><a href="https://giedrius.blog/2019/05/11/push-vs-pull-in-monitoring-systems/?ref=blog.realvarez.com">Push Vs. Pull In Monitoring Systems &#x2013; Giedrius Statkevi&#x10D;ius</a></p>
<p><a href="https://www.splunk.com/en_us/blog/learn/sre-metrics-four-golden-signals-of-monitoring.html?ref=blog.realvarez.com">SRE Metrics: Four Golden Signals of Monitoring</a></p>
<p><a href="https://www.oreilly.com/library/view/learning-modern-linux/9781098108939/?ref=blog.realvarez.com">Learning Modern Linux</a></p>
<p><a href="https://www.oreilly.com/library/view/release-it/9781680500264/?ref=blog.realvarez.com">Release It!</a></p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[How much do my Amazon EKS nodes cost?]]></title><description><![CDATA[Use instance tags to analyze EC2 spend in AWS Cost Explorer. ]]></description><link>https://realvz.github.io/blog/how-much-do-my-amazon-eks-nodes-cost/</link><guid isPermaLink="false">62a75a92f171aa00011844f5</guid><category><![CDATA[AWS]]></category><category><![CDATA[Containers]]></category><category><![CDATA[Amazon EKS]]></category><category><![CDATA[Kubernetes]]></category><category><![CDATA[Cloud]]></category><category><![CDATA[Cost]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Mon, 02 May 2022 19:21:47 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1626266061368-46a8f578ddd6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhbGN1bGF0b3J8ZW58MHx8fHwxNjUxNTE5NDE3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1626266061368-46a8f578ddd6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhbGN1bGF0b3J8ZW58MHx8fHwxNjUxNTE5NDE3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="How much do my Amazon EKS nodes cost?"><p>If you want to know the cost of running Amazon EKS nodes, AWS Cost Explorer is your friend.</p>
<p><a href="https://aws.amazon.com/aws-cost-management/aws-cost-explorer/?ref=blog.realvarez.com">AWS Cost Explorer</a> helps you analyze your AWS bill. Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.</p>
<p>Cost Explorer supports breaking down costs using <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html?ref=blog.realvarez.com">tags</a>. A tag is a label that you assign to an AWS resource. Tags enable you to categorize your AWS resources by, for example, purpose, owner, or environment.</p>
<p>Kubernetes cluster deployment tools like <code>eksctl</code> automatically add default tags to EC2 instances that are part of a managed node group. Customer can view tags attached to their nodes in the AWS Management Console.</p>
<p>To view the tags attached to your EKS nodes, go to the AWS Management Console and inspect the tags. You can also use the <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html?ref=blog.realvarez.com"><code>DescribeTags</code> API</a> to query tags using AWS CLI or SDK.</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/99997738-88A8-4420-9881-286553E32476_2/hqNzdOTLsvdLYOyrPn70lF2cToHkSbhIDtm4xr8RA08z/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>Pick an EC2 instance that&apos;s part of your EKS cluster, note its tags, select a tag, like <code>eks:cluster-name</code>, and verify that all nodes have that label. You will later use this tag to separate the costs of Kubernetes nodes from the rest of EC2 usage in your account(s).</p>
<h2 id="activate-tags-in-billing">Activate tags in billing</h2>
<p>Once you have identified the tag using which you are going to identify EKS EC2 nodes, you&#x2019;ll have to activate that tag in the AWS Billing and Cost Management console. You&#x2019;ll need <a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html?ref=blog.realvarez.com">privileges</a> to make changes.</p>
<p>To activate your tags using the AWS Management Console:</p>
<ol>
<li>Sign in to the AWS Management Console and open the AWS Billing console at <a href="https://console.aws.amazon.com/billing/?ref=blog.realvarez.com">https://console.aws.amazon.com/billing/</a>.</li>
<li>In the navigation pane, choose Cost Allocation Tags.</li>
<li>Select the tags that you want to activate.</li>
<li>Choose Activate.</li>
</ol>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/263EE126-DAD2-4F52-856E-6F065BF89E53_2/AwhushwYYUHGexRRGMqTseZBAfi0u2rQP8k2k8xeNpoz/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>After you select your tags for activation, it can take <strong>up to 24 hours</strong> for tags to activate.</p>
<p>Billing has AWS-generated cost allocation tags that customers can use to identify costs.</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/0C3F9990-AB32-4876-85EB-CE7F7D9E4C83/123DF0AF-8C9D-4625-8816-FF6A2511B596_2/lHrGroWK3tRZFJVuLqOdx2xPWKEEADFIo5teWiGeIbgz/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<h2 id="view-charges-in-aws-cost-explorer">View charges in AWS Cost Explorer</h2>
<p>Navigate to AWS Cost Explorer and create a new report</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/3B13ED5A-B51C-4109-8D51-E3BC9187BF8A_2/dv6xOgqDiX8lQMHmPz0yqeySZ6OgPqEBAqRLyEifu3gz/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>In the next screen, choose <strong>Cost and usage</strong>. Then scroll to the bottom of page and choose <strong>create report.</strong></p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/0C3F9990-AB32-4876-85EB-CE7F7D9E4C83/75A8974D-92E5-487B-8A66-F1DA634E81B0_2/aoYH9yyFtz8vHHrxMuthXXXaBlGYAuLdeUxbPeTqDCIz/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>In the next screen, configure a <strong>Service</strong> filter, select EC2-Instances, and apply filter.</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/0C3F9990-AB32-4876-85EB-CE7F7D9E4C83/A360630B-8E50-42EE-8245-C5AADFA5BFFC_2/wmtRAMwgAJWxXCTAMjeTGPogUhEmOdiBnRxOuTylyuYz/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>Create a <strong>Tag</strong> filter, select the tag you activated in billing, and apply. Be sure to select only <code>true</code>.</p>
<p>In my case, my nodes had the <code>eks:cluster-name</code> tag.</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/7B8C4159-87D5-4FBF-847F-0AA3DB34730C/22D7482D-E383-43BC-B123-5D4C805D7C52_2/PPl1BfJ7puxg0H4lPCLqyxpX58r4cEAO0R7iTzLRsLIz/Image" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>Now Cost Explorer shows just the charges for my cluster.</p>
<p><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/AE2DDB1D-DD37-409F-81C6-B5F447210571_2/YpYzKtND10nuEyoY0qifZWzkxyyX31CW511EIjI02xwz/Image.jpeg" alt="How much do my Amazon EKS nodes cost?" loading="lazy"></p>
<p>Now I know that in Feb &#x2019;22, I paid roughly $3000 for the nodes that are part of the cluster named &#x201C;Socrates.&#x201D;</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Book Review: 12 Rules for Life - Jordan Peterson]]></title><description><![CDATA[My review of Clinical Psychologist Jordan Peterson's seminal book. ]]></description><link>https://realvz.github.io/blog/book-review/</link><guid isPermaLink="false">62a75a92f171aa00011844f3</guid><category><![CDATA[Psychology]]></category><category><![CDATA[Book Review]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Mon, 18 Apr 2022 07:02:36 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1597149305638-fef8c2b1981b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGpvcmRhbiUyMHBldGVyc29ufGVufDB8fHx8MTY1MDI2NTEzOQ&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/7D92D31F-484C-458F-87D2-0566989F03F7/211FD1DF-D78B-428F-ABA0-87A479847E55_2/HssPmIUUNFLxoMxeEJdRiyx6znxxqEe0b2Z6hyUsnEwz/Image" class="kg-image" alt="Book Review: 12 Rules for Life - Jordan Peterson" loading="lazy"></figure><img src="https://images.unsplash.com/photo-1597149305638-fef8c2b1981b?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGpvcmRhbiUyMHBldGVyc29ufGVufDB8fHx8MTY1MDI2NTEzOQ&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Book Review: 12 Rules for Life - Jordan Peterson"><p>Of all the books I have read, this has to be one of the most polarizing. I must admit that I started this book with a heavy bias against the author, thanks to his infamy on progressive subs on Reddit. The first few pages didn&#x2019;t make it any easier; the prologue was filled with things that you&#x2019;d see on a Twitter &#x201C;#ShitDonaldTrumpSays&#x201D; thread.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/7D92D31F-484C-458F-87D2-0566989F03F7/7323E51C-64C0-4940-8063-8FBE31429A1A_2/pKAZNxYoyn4QySBCTI4laKPof2JRUnKpuvWfeMLDt3Yz/Image" class="kg-image" alt="Book Review: 12 Rules for Life - Jordan Peterson" loading="lazy"></figure><p>In the beginning, I found author&#x2019;s self-righteous tone off-putting. I admit that I skipped pages whenever Bible was the sole exemplary. Gradually, though, I found myself agreeing more and more with the author, albeit at a very fundamental level. When I <em> </em>agreed, I still found his method of deduction dubious and massively sexist.</p><p>His points of reference, when it comes to sexism, are so flawed (in my humble opinion) that the reader is tempted to discount the author entirely, but if you wade through much of his ramblings, you may find a learning or two. At this point, you are better off reading a more concise self-help book because the author loves to ramble a lot. Which is fine for readers that don&#x2019;t care about the length of the book. But if you&#x2019;re reading this book for self-improvement, 1/ this is not the most well-written self-help book (its a good book though) and 2/ the author&#x2019;s tone is polarizing, for almost every ethnicity, ideology, or sex.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/7D92D31F-484C-458F-87D2-0566989F03F7/7BC4C43E-4943-4F3B-BC14-2523A2464F8E_2/Dl5UyQmnDraSK2dQvHjNyu7RHZyxsYrodUox2ukUcAAz/Image" class="kg-image" alt="Book Review: 12 Rules for Life - Jordan Peterson" loading="lazy"></figure><p>The book advocates a lot about masculinity. It&#x2019;s innate aggression. How women, who failed as the leaders of ancient society, are on a constant pursuit to introduce feminine qualities such as &#x201C;agreeableness&#x201D; (who&#x2019;d have thunk &#x1F92F;) to rein in the masculine aggression. The aggression should be fostered as it pushes young men to innovate so that they can get ride their bikes faster. Is the author saying that only men are capable of doing stupid stuff you&apos;d generally see on MTV Jackass &#x1F914;.</p><p>Time and time again, the author justifies violence or aggression. I have a feeling the author and Will Smith will be good friends.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/7D92D31F-484C-458F-87D2-0566989F03F7/19272A08-8F3E-48E8-92FF-AD3DF1E6E7E4_2/l71ZKFKmFPX6KZ6efXLSFUfZgIHrFg99ulqXCRw2Iykz/Image" class="kg-image" alt="Book Review: 12 Rules for Life - Jordan Peterson" loading="lazy"></figure><p>I find it interesting (to say the least) that despite being a psychologist, the author has things like to say:</p><blockquote>&#x201C;Girls can win by winning in their own hierarchy&#x2014;by being good at what girls value, as girls. They can add to this victory by winning in the boys&#x2019; hierarchy. Boys, however, can only win by winning in the male hierarchy. They will lose status, among girls and boys, by being good at what girls value. It costs them in reputation among the boys, and in attractiveness among the girls. <strong>Girls aren&#x2019;t attracted to boys who are their friends, even though they might like them, whatever that means.</strong> They are attracted to boys who win status contests with other boys. If you&#x2019;re male, however, you just can&#x2019;t hammer a female as hard as you would a male. Boys can&#x2019;t (won&#x2019;t) play truly competitive games with girls. It isn&#x2019;t clear how they can win. As the game turns into a girls&#x2019; game, therefore, the boys leave.</blockquote><p>So, basically rich woman should compare resumes before forming romantic ties &#x1F44D;&#x1F3FC;. In my view the author&apos;s message is not tha<em>t se</em>xist, but it&apos;s definitely worded to encourage misinterpretations. Great writing or flawed ideology &#x2696;&#xFE0F;?</p><blockquote>&#x201C;If they&#x2019;re healthy, women don&#x2019;t want boys. They want men. They want someone to contend with; someone to grapple with. If they&#x2019;re tough, they want someone tougher. If they&#x2019;re smart, they want someone smarter. They desire someone who brings to the table something they can&#x2019;t already provide. This often makes it hard for tough, smart, attractive women to find mates: there just aren&#x2019;t that many men around who can outclass them enough to be considered desirable &#x201D;</blockquote><blockquote>Men enforce a code of behavior on each other, when working together. Do your work. Pull your weight. Stay awake and pay attention. Don&#x2019;t whine or be touchy. Stand up for your friends. Don&#x2019;t suck up and don&#x2019;t snitch. Don&#x2019;t, in the immortal words of Arnold Schwarzenegger, be a <strong>girlie man</strong>. Don&#x2019;t be dependent. At all. Ever. Period. The harassment that is part of acceptance on a working crew is a test: are you tough, entertaining, competent and reliable? If not, go away. Simple as that. We don&#x2019;t need to feel sorry for you. We don&#x2019;t want to put up with your narcissism, and we don&#x2019;t want to do your work.&#x201D;</blockquote><p>I was not surprised to learn that the author doesn&#x2019;t believe in feminism. He goes on to say that women have little to contribute in many ways</p><blockquote>&#x201C;Furthermore, even if women contributed nothing substantial to art, literature and the sciences prior to the 1960s and the feminist revolution (which is not something I believe), then the role they played raising children and working on the farms was still instrumental in raising boys and freeing up men&#x2014;a very few men&#x2014;so that humanity could propagate itself and strive forward.&#x201D;</blockquote><p>In his opinion there have been many men like the tampon king of India that have advocated for women. I am still unclear how that justifies not believing in feminism but &#x1F937;&#x1F3FD;&#x200D;&#x2642;&#xFE0F;.</p><p>I did enjoy author&#x2019;s take on classics like Dostoevsky, Sartre, and other western European philosophers. Although, I found his interoperation of Hansel and Gretel hilarious &#x1F602;.</p>]]></content:encoded></item><item><title><![CDATA[Recap: Amazon ECS supports warm pools for Amazon EC2 Auto Scaling]]></title><description><![CDATA[<p>Containers not scaling fast enough? check out EC2 Auto Scaling Warm Pools.</p><h2 id="%F0%9F%A4%94what%E2%80%99s-a-warm-pool">&#x1F914;What&#x2019;s a warm pool?</h2><p>It&#x2019;s an EC2 Auto Scaling feature that reduces scale-out latency by maintaining a pool of pre-initialized instances ready to be placed into service. EC2 Auto Scaling Warm Pools works</p>]]></description><link>https://realvz.github.io/blog/recap-amazon-ecs-supports-warm-pools-for-amazon-ec2-auto-scaling/</link><guid isPermaLink="false">62a75a92f171aa00011844f6</guid><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Thu, 14 Apr 2022 19:35:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1625844393947-27ed2fa505af?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIxfHx3YXJtJTIwcG9vbHxlbnwwfHx8fDE2NTE1MjAxMTQ&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1625844393947-27ed2fa505af?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIxfHx3YXJtJTIwcG9vbHxlbnwwfHx8fDE2NTE1MjAxMTQ&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Recap: Amazon ECS supports warm pools for Amazon EC2 Auto Scaling"><p>Containers not scaling fast enough? check out EC2 Auto Scaling Warm Pools.</p><h2 id="%F0%9F%A4%94what%E2%80%99s-a-warm-pool">&#x1F914;What&#x2019;s a warm pool?</h2><p>It&#x2019;s an EC2 Auto Scaling feature that reduces scale-out latency by maintaining a pool of pre-initialized instances ready to be placed into service. EC2 Auto Scaling Warm Pools works by launching a configured number of EC2 instances in the background, allowing any lengthy application initialization processes to run as necessary, and then stopping those instances until they are needed.</p><h2 id="%F0%9F%A4%A8why-shouldn%E2%80%99t-every-one-get-a-warm-pool">&#x1F928;Why shouldn&#x2019;t every one get a warm pool?</h2><p>Warm pool is suitable for applications with long unavoidable initialization times.</p><p>Creating a warm pool when it&apos;s not required can lead to unnecessary costs. If your first boot time does not cause noticeable latency issues for your application, there probably isn&apos;t a need for you to use a warm pool.</p><h2 id="%F0%9F%A7%90what-are-warm-pool-limitations">&#x1F9D0;What are warm pool limitations?</h2><ul><li>You cannot add a warm pool to Auto Scaling groups that have a mixed instances policy or that launch Spot Instances.</li><li>Amazon EC2 Auto Scaling can put an instance in a Stopped or Hibernated state only if it has an Amazon EBS volume as its root device. Instances that use instance stores for the root device cannot be stopped or hibernated.</li><li>Amazon EC2 Auto Scaling can put an instance in a Hibernated state only if meets all of the requirements listed in the Hibernation prerequisites topic in the Amazon EC2 User Guide for Linux Instances.</li><li>If your warm pool is depleted when there is a scale-out event, instances will launch directly into the Auto Scaling group (a cold start). You could also experience cold starts if an Availability Zone is out of capacity.</li></ul><h2 id="how-can-i-use-warm-pool-with-amazon-ecs">How can I use warm pool with Amazon ECS?</h2><p>To use warm pools with your Amazon ECS cluster, set the ECS_WARM_POOLS_CHECK agent configuration variable to true in the User data field of your Amazon EC2 Auto Scaling group launch template. The following shows an example of how the agent configuration variable can be specified in the User data field of an Amazon EC2 launch template.</p><pre><code class="language-bash">#!/bin/bash
cat &lt;&lt;&apos;EOF&apos; &gt;&gt; /etc/ecs/ecs.config
ECS_CLUSTER=MyCluster
ECS_WARM_POOLS_CHECK=true
EOF
</code></pre><p>Check out the announcement <a href="https://aws.amazon.com/about-aws/whats-new/2022/03/amazon-ecs-supports-warm-pools-amazon-ec2-auto-scaling/?ref=blog.realvarez.com">here</a>.</p><p>&#x1F4CF;Happy scaling!</p>]]></content:encoded></item><item><title><![CDATA[What are Rollups in Blockchain Networks 🍣]]></title><description><![CDATA[Rollups offer faster and cheaper transactions for dApp developers and their customers. This post summarizes my research on rollups, and a few things dApp developers should know when picking the right blockchain protocol.]]></description><link>https://realvz.github.io/blog/what-are-rollups/</link><guid isPermaLink="false">62a75a92f171aa00011844f2</guid><category><![CDATA[Ethereum]]></category><category><![CDATA[Blockchain]]></category><category><![CDATA[web3]]></category><dc:creator><![CDATA[Re Alvarez Parmar]]></dc:creator><pubDate>Mon, 11 Apr 2022 07:45:03 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1638818834413-c3769cfdd561?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxldGhlcmV1bXxlbnwwfHx8fDE2NDk2NjMwMjM&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1638818834413-c3769cfdd561?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDIwfHxldGhlcmV1bXxlbnwwfHx8fDE2NDk2NjMwMjM&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="What are Rollups in Blockchain Networks &#x1F363;"><p>Increasing the transactional throughput of public blockchains is a key focus for blockchain researchers today. <a href="https://eips.ethereum.org/EIPS/eip-4844?ref=blog.realvarez.com">EIP-4844</a> just came out, and it&apos;s old news that rollups will play a huge role in the future of scaling Ethereum. Ethereum&apos;s co-founder Vitalik Buterin described the concept of rollups back in 2014. Last year, Vitalik <a href="https://vitalik.ca/general/2021/01/05/rollup.html?ref=blog.realvarez.com">claimed rollups to be the &#x201C;only choice&quot; for making gas fees more affordable</a>.</p><p>Rollups offer faster and cheaper transactions for dApp developers and their customers. This post summarizes my research on rollups, and a few things dApp developers should know when picking the right blockchain protocol.</p><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/1C445B41-B14F-4E8F-9208-E6D7E9565D52/86D1780C-B4FD-4AF8-BF0B-256F24627357_2/AxrzzGRiNYaxyhbjmnipvXDP38mulKJn0G1WKVtYy6Iz/Image.jpeg" class="kg-image" alt="What are Rollups in Blockchain Networks &#x1F363;" loading="lazy"></figure><h2 id="why-do-we-need-rollups-%F0%9F%9B%BC">Why do we need rollups? &#x1F6FC;</h2><blockquote>Rollups are in the short and medium-term, and possibly in the long term, the only trustless scaling solution for Ethereum.</blockquote><p>Thanks Vitalik for that intro. If you&#x2019;re familiar with Ethereum, you also know about the current gas prices crises. Basically, if you transfer coins on Ethereum, be prepared to fork over up to $10 in fees to send $1. The last sentence may not be an exaggeration. &#x1F614;</p><p>Rollups are a layer 2 scaling solution to drastically reduce the gas price on the Ethereum mainnet. Rollups are supposed to provide a way to reduce the costs and latency of decentralized applications (dApps) for users and developers.</p><p>In layer 2 scaling solutions, web3 apps send transactions to nodes that are part of the layer 2 network, then the network batches transactions into groups before <em>anchoring (publishing)</em> them to layer 1, after which they are secured by layer 1 since they are publicly verifiable and cannot be altered. Thus rollups offer faster execution by executing transactions off-chain and publishing the proof of transactions on-chain.</p><blockquote>Rollups move computation (and state storage) off-chain, but keep some data per transaction on-chain.</blockquote><p>A rolledup transaction could include tens of thousands of transactions, which means tens of thousands of transactions can be recorded on the mainchain for the price of one. Using compression algorithms, the more layer 2 transactions you can bundle in a single layer 1 transaction, the cheaper it is to store proof of transactions.</p><blockquote><a href="https://jsidhu.medium.com/the-ultimate-guide-to-rollups-f8c075571770?ref=blog.realvarez.com">Jag Sidhu </a>writes &#x201C;some Ethereum engineers got these individual account updates down to a few bytes (8&#x2013;12 bytes depending on the implementation) which means that a block with 1 megabyte of bandwidth would be able to roughly process 83k &#x2014; 125k account adjustments per block and around 5500 to 8300 TPS theoretically assuming 15 second block times.&#x201D;</blockquote><figure class="kg-card kg-image-card"><img src="https://res.craft.do/user/full/9d54cc03-adfe-f72f-3389-565eb7356d1d/doc/1C445B41-B14F-4E8F-9208-E6D7E9565D52/3B62E811-91E8-408B-9B3C-574AA058D50D_2/90FGxI1BElqAd7GnK8ViyaIAo4JYzzIgsNZkhbmqXKwz/Image.jpeg" class="kg-image" alt="What are Rollups in Blockchain Networks &#x1F363;" loading="lazy"></figure><p><a href="https://wwz.unibas.ch/fileadmin/user_upload/wwz/00_Professuren/Schaer_DLTFintech/Lehre/Tobias_Schaffner_Masterthesis.pdf?ref=blog.realvarez.com">Image source</a></p><h2 id="types-of-rollups">Types of rollups</h2><p>The paper subtly categorizes rollups into two prominent categories: Arbitrum and Optimism with fees that are ~3-8x lower gas fees than L1 and ZK-rollups, with ~40-100x lower gas fees than Ethereum mainnet.</p><p>So, what&#x2019;s the difference between Arbitrum and Optimism that provide single-digit gains than ZK-rollups with triple-digit gains? That&#x2019;s because there are two types of layer 2 scaling solutions: Optimistic and ZK.</p><h3 id="optimistic-rollups">Optimistic rollups</h3><p><a href="https://offchainlabs.com/?ref=blog.realvarez.com">Arbitrium</a> and <a href="https://www.optimism.io/?ref=blog.realvarez.com">Optimism</a> are layer 2 protocols that use &#x201C;optimistic rollup&#x201D; (OR) to scale Ethereum. An optimistic rollup network assumes that transactions are valid by default and only performs calculations, via a fraud proof, in the event of a challenge.</p><p>In other words, when an application transacts on an optimistic rollup network like Arbitrum, the actual transfer of funds (from <em>accountA</em> to <em>accountB</em>) happens on the Arbitrum. The transaction is then published on Ethereum mainnet.</p><p>Remember that an optimistic rollup network assumes all transactions are valid, at least initially. So what happens if a transaction in invalid?</p><p>This is indeed a problem with optimistic rollups. Because, every transaction is assumed valid, optimistic rollups have a <em>withdrawal time (7-14 days)</em> constraints while the network waits for <em>someone else</em> to challenge the state of the network.</p><p>Optimistic Rollups rely on fraud proofs to avoid re-computations. The state is proposed to Ethereum by a &#x201C;bonded&#x201D; actor. Anyone who wants to challenge the actor may claim a bounty by proving that the state update is inaccurate. To accomplish this, the challenger must provide the data required by the smart contract to prove the inaccuracy. <a href="https://threadreaderapp.com/thread/1395812308451004419.html?ref=blog.realvarez.com">This thread</a> goes over the key difference between Optimism and Arbitrum fraud proof mechanism.</p><p>ZK-rollups don&#x2019;t have the withdrawal time constraint because they include a validity proof.</p><h3 id="zk-rollups">ZK Rollups</h3><p>For Ethereum &#x2014; and EVM compatible chains &#x2014; to become world&#x2019;s next distributed computing platform, gas prices have to be massively reduced, until it is cheaper to do things at internet scale. ZK rollups (ZKR) promise could be the key to achieving that level of scalability.</p><p>ZK rollups like <a href="https://zksync.io/?ref=blog.realvarez.com">zkSync</a> are popular because they don&#x2019;t have the <em>withdrawal time</em> problem that optimistic rollups do. Withdrawal times in zkSync, a ZK rollup live on Ethereum mainnet, are 10 minutes to 7 hours during low usage. Moreover, ZK rollups gets cheaper and faster as the usage increases, so in the future things will become faster.</p><h2 id="but-what-does-zk-stand-for">But, what does ZK stand for?</h2><p>ZK rollups are based on the concept of provers and verifiers. ZK stands for Zero Knowledge.</p><p>ZKR &quot;roll-up&quot; off-chain transactions and generate a cryptographic proof known as a zk-SNARK. The acronym <a href="https://en.wikipedia.org/wiki/Non-interactive_zero-knowledge_proof?ref=blog.realvarez.com">zk-SNARK</a> stands for &#x201C;Zero-Knowledge Succinct Non-Interactive Argument of Knowledge&quot;. The zk-SNARK is the proof of validity of the transactions in the form of a hash and is eventually placed on the main chain.</p><p>A special ZK Rollup smart contract, which resides on Layer 1, maintains the status of the transfers made on rollup chain. The status can only be updated with a validity card; the zk-SNARK. The zk-SNARK is a hash that represents the blockchain&apos;s validity status.</p><p>&#x201C;Zero-knowledge&#x201D; proofs allow one party (the prover) to prove to another (the verifier) that a statement is true without revealing any information beyond the validity of the statement itself. For example, given the hash of a random number, the prover could convince the verifier that there indeed exists a number with this hash value, while disclosing what that random number is.</p><blockquote>In a zero-knowledge &#x201C;Proof of Knowledge&#x201D; the prover can convince the verifier not only that the number exists, but that they in fact know such a number &#x2013; again, without revealing any information about the number.</blockquote><p>zk-SNARK&#x2019;s succinct proofs are only a few hundred bytes and can be verified within a few milliseconds. The ZK proof mathematically proves that no fraud has occurred.</p><p><a href="https://zksync.io/?ref=blog.realvarez.com">zkSync</a> is a ZKR live on Ethereum mainnet. <a href="https://www.immutable.com/?ref=blog.realvarez.com">Immutable X</a> and <a href="https://loopring.io/?ref=blog.realvarez.com#/">Loopring</a> also use ZKR. <a href="https://z.cash/?ref=blog.realvarez.com">Zcash</a> is the first widespread application of zk-SNARKs. Polygon is focused on Zero-Knowledge (ZK) cryptography as the end game for blockchain scaling. There&apos;s a lot of innovation happening in this space. <a href="https://l2beat.com/?ref=blog.realvarez.com">L2beat.com</a> provides details about Ethereum layer 2 scaling solutions.</p><h3 id="zkr-core-components">ZKR core components</h3><p>ZKRs execute transactions on sidechain and roll them on the mainchain. ZKR use two transactors and relayers to achieve this.</p><ul><li><strong>Transactor</strong> create and broadcast transaction data (indexed address, value, network fee, and nonce) to the network. Transactor corresponds to an external account on Ethereum. Smart contracts then record addresses to one Merkle Tree and the transaction value to another.</li><li><strong>Relayers</strong> collect a large number of transactions creating rollups. Relayers generate the ZK proof that creates the blockchain <em>state</em> before and after each transaction. The resulting changes reach the mainchain in a verifiable hash. Although anyone can become a relayer, you must first stake their cryptocurrency in a smart contract to ensure honesty.</li></ul><blockquote>This &#x201C;<strong>state</strong>&#x201D; is essentially a database which represents new balances and adjustments to accounts as users transact with their accounts inside of the rollup</blockquote><h3 id="how-do-rollups-reduce-gas">How do rollups reduce gas?</h3><p>Rollups don&#x2019;t actually reduce the gas on Ethereum. Recall that a rollup is a layer 2 sidechain; when using a rollup, you won&#x2019;t be sending transactions on Ethereum mainnet; instead transactions will be submitted to the L2.</p><p>Users of a dApp running the ZK-Rollup scheme will pay less in transaction fees.</p><h2 id="are-optimistic-rollups-a-temporary-solution">Are Optimistic Rollups a temporary solution?</h2><p>This seems to be a common question in the community. If ZKR are faster, then why even bother with OR?</p><p>Optimistic rollups have a first-mover advantage. First of all, the main reason why OR were more popular in the past was because until recently ZKRs didn&apos;t support Solidity smart contracts. ZKRs have to generate validation proofs, and the earliest iterations were not EVM and Solidity compatible. That changed in 2021. Now you can take your Solidity smart contract and deploy it on a ZKR with a few (relatively minor) changes.</p><p>On Feb 2022, zkSync 2.0 became available on Ethereum&#x2019;s testnet. <a href="https://docs.zksync.io/zkevm/?ref=blog.realvarez.com">zkEVM</a> is a virtual machine that executes smart contracts in a way that is compatible with zero-knowledge-proof computation.</p><p>Only time will tell who wins.</p><h3 id="how-will-the-merge-affect-this">How will the merge affect this?</h3><p>Simply put, it will not. I&#x2019;ll provide a detailed answer in another post.</p><h2 id="topics-we-skipped">Topics we skipped</h2><p>In optimistic rollups, when transactions are ready to be rolled up, a <strong>sequencer</strong> is a specially designated full node that can control the ordering of transactions. Sequencers bundle transactions and submit both the transaction data and the new L2 state root to L1. Kyle Charbonnet has explained Optimism&apos;s optimistic rollup implementation in detail <a href="https://medium.com/privacy-scaling-explorations/an-introduction-to-optimisms-optimistic-rollup-8450f22629e8?ref=blog.realvarez.com">here</a>.</p><p><a href="https://starkware.co/stark/?ref=blog.realvarez.com"><strong>ZK-STARK</strong> (Zero-Knowledge Scalable Transparent ARguments of Knowledge)</a>. The proof system used in ZK-SNARK requires a trusted party, or parties, to initially set up the ZK proof system. A dishonest trusted party could compromise the privacy of the system. ZK-STARKS improve on this technology by removing the need for a trusted setup.</p><h2 id="conclusion">Conclusion</h2><p>Blockchain is a fast-moving space. Millions of dollars continue to be funneled into building scalable future blockchain networks. It&#x2019;s hard to tell if ZKRs will be the silver bullet to address Ethereum&#x2019;s data availability and scaling problems. In the short term, it does look like ZKRs are a step in the right direction.</p><hr><h3 id="references">References</h3><p><a href="https://www.reddit.com/r/CryptoCurrency/comments/nctot7/defi_explained_zk_rollups/?ref=blog.realvarez.com">DeFi Explained: ZK Rollups</a></p><p><a href="https://www.reddit.com/r/CryptoCurrency/comments/rvktc1/what_are_zkrollups_and_why_theyre_the_best/?ref=blog.realvarez.com">What are ZK-Rollups and why they&apos;re the best investment you can make in 2022.</a></p><p><a href="https://jsidhu.medium.com/the-ultimate-guide-to-rollups-f8c075571770?ref=blog.realvarez.com">The Ultimate Guide to Rollups</a></p><p><a href="https://curve.substack.com/p/february-20-2021-optimistic-vs-zk?s=r&amp;ref=blog.realvarez.com">February 20, 2021: Optimistic vs ZK-Rollups, ELI5 &#x1F9D2;&#x1F9D1;&#x200D;&#x1F3EB;</a></p><p><a href="https://finematics.com/rollups-explained/?ref=blog.realvarez.com">Rollups &#x2013; The Ultimate Ethereum Scaling Solution &#x2013; Finematics</a></p><p><a href="https://vitalik.ca/general/2021/01/05/rollup.html?ref=blog.realvarez.com">An Incomplete Guide to Rollups</a></p><p><a href="https://research.paradigm.xyz/optimism?ref=blog.realvarez.com">How does Optimism&#x2019;s Rollup really work?</a></p><p><a href="https://z.cash/technology/zksnarks/?ref=blog.realvarez.com">What are zk-SNARKs? | Zcash</a></p><ul><li><a href="https://wwz.unibas.ch/fileadmin/user_upload/wwz/00_Professuren/Schaer_DLTFintech/Lehre/Tobias_Schaffner_Masterthesis.pdf?ref=blog.realvarez.com">Scaling Public Blockchains</a><br><a href="https://medium.com/privacy-scaling-explorations/an-introduction-to-optimisms-optimistic-rollup-8450f22629e8?ref=blog.realvarez.com">An Introduction to Optimism&#x2019;s Optimistic Rollup</a></li></ul>]]></content:encoded></item></channel></rss>